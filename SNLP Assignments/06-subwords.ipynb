{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f5b828996a358703dfbada159de40c2",
     "grade": false,
     "grade_id": "cell-a74ccc5dcde03d2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 6: Subword segmentation\n",
    "\n",
    "## Released: 28.02.2023 at 14:30\n",
    "## Deadline: 13.03.2023 at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9865939ae61c4207f5e1e127fb027902",
     "grade": false,
     "grade_id": "cell-bd0631ed211d7cca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "We've already talked about the problem of segmenting text into appropriate units (tokenization). Back then, it was words that were considered as those units, but there are other units that you should probably explore as well: characters and **subwords**. In this assignment, we're going to focus on **tokenization into subwords**.\n",
    "\n",
    "The motivation to segmenting words further into smaller elements comes from morphology, where such elements are called **morphemes**. A **morpheme** is defined as the smallest meaning-bearing unit of a language. For example, the word *unpredictable* contains three morphemes: *un*, *predict* and *able*. As you can see, many morphemes are not unique to one word, they are elements that are regularly seen in other words too. For example, *un* in *unhappy*, *predict* in *predictive*, and *able* in *comfortable*. Thus, just as sentences are constructed from words, words are constructed from morphemes. \n",
    "\n",
    "Segmenting into morphemes (especially in languages with rich morphology) helps to avoid the problem of out-of-vocabulary (OOV) words in text corpora. For example, if our training corpus contains *cool*, *cool-est* and *dumb-er* when the new word *cooler* comes in, it can be segmented into *cool-er* and understood based on its constituent subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e92be7689663906ea2286ac72f616554",
     "grade": false,
     "grade_id": "cell-5f9b8e119097a2a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Task 1: BPE](#task_1)\n",
    "    * [Step 1.1](#subtask_1_1)\n",
    "        * [Step 1.1.1: Collecting word counts](#subtask_1_1_1)\n",
    "        * [Step 1.1.2: Convering words to characters](#subtask_1_1_2)\n",
    "    * [Step 1.2: Collecting symbol pairs frequencies](#subtask_1_2)\n",
    "    * [Step 1.3: Merging the most frequent pair](#subtask_1_3)\n",
    "    * [Step 1.4: Combining steps 1-3 together](#subtask_1_4)\n",
    "    * [Step 1.5-7: Segmenting a corpus](#subtask_1_5)\n",
    "* [Task 2: ANALYZE SEGMENTATIONS](#task_2)\n",
    "    * [Step 2.1: Count word OOV](#subtask_2_1)\n",
    "    * [Step 2.2: Count subword OOV](#subtask_2_2)\n",
    "    * [Step 2.3: Does the segmentation make sense?](#subtask_2_3)\n",
    "    * [Step 2.4: Your thoughts](#subtask_2_4)\n",
    "* [Checklist](#checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53e8c70ef1f6052446549f6d60bb70e7",
     "grade": false,
     "grade_id": "cell-2cbd322d57c4b67a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 1 <a class=\"anchor\" id=\"task_1\"></a>\n",
    "## BPE\n",
    "One approach to tokenization into subwords is based on the **byte pair encoding** (**BPE**) algorithm for data compression. Starting from single characters as the subwords, it iteratively merges frequent pairs of subwords forming new subwords. Assuming morphemes are frequently repeated substrings, this method should often merge symbols into morphemes instead of into other, meaningless character sequences. The algorithm is applied only inside words (there are no merges across word boundaries). \n",
    "\n",
    "**BPE** algorithm begins with its vocabulary being a set of characters seen in the training corpus. Each word in the corpus is represented as a sequence of characters plus a special end-of-word symbol '_'. At each iteration step $k_i$, the algorithm counts the number of symbol pairs, finds the most frequent pair ['A','B'] and replaces it with the new merged symbol ‘AB’. The algorithm stops when it's done $k$ merges ($k$ is a parameter of the algorithm). The algorithm begins with the set of symbols equal to the set of characters. The resulting symbol set should have the original set of characters plus $k$ new symbols. \n",
    "\n",
    "To obtain a subword vocabulary with **BPE**, you should take the following steps:\n",
    "* STEP 1: tokenize a training corpus into words and collect frequency statistics of word tokens in the training corpus. Additionally, represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 2: count the frequencies of symbol pairs.\n",
    "* STEP 3: replace every occurrence of the most frequent pair ['A', 'B'] with the new merged symbol 'AB'.\n",
    "* STEP 4: repeat STEPs 2-3 $k-1$ times more.\n",
    "\n",
    "To segment a corpus using the vocabulary, you should:\n",
    "* STEP 5: tokenize a test corpus into words (with the same tokenization algorithm as in training).\n",
    "* STEP 6: represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 7: for every word apply each merge operation in the order they were learned, and return the tokenized text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a4560f44b39d4596fb668023dbf7a5e",
     "grade": false,
     "grade_id": "cell-fc78b008631ec01d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 <a class=\"anchor\" id=\"subtask_1_1\"></a>\n",
    "### STEP 1\n",
    "### 1.1.1  <a class=\"anchor\" id=\"subtask_1_1_1\"></a>\n",
    "### Collecting word counts (1 Point)\n",
    "\n",
    "Write a function that reads a text as one string from a file, tokenizes it into words by whitespaces and collects the word frequencies. It should return a dictionary of words and their raw counts in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1effda1a6f0f3cd62ff99a153f091f3",
     "grade": false,
     "grade_id": "cell-1b45cebfbb7868fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def collect_word_counts(file_name):\n",
    "    \"\"\"\n",
    "    Takes in a path to a text file, reads the file, splits it into words by whitespaces, \n",
    "    and then counts the words' frequencies.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    file_name : string\n",
    "            a path to a training corpus as a string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word_counts : dictionary\n",
    "            a dictionary of word counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    fp=open(file_name,'r')\n",
    "    text=fp.read()\n",
    "    fp.close()\n",
    "    tokens=text.split()\n",
    "    dic=collections.Counter(tokens)\n",
    "    \n",
    "\n",
    "   \n",
    "    word_counts=dict(dic)\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bc11c24648325ed3a83e7f8976d526e",
     "grade": true,
     "grade_id": "cell-ac8d751900ffa327",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_corpus_path = \"/coursedata/06-subwords/dummy_corpus.txt\"\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(collect_word_counts(dummy_corpus_path)), dict)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['low'], 5)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['lowest'], 2)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['new'], 2)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['newer'], 6)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['WIDer'], 3)\n",
    "\n",
    "\n",
    "# A SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "gum_train_path = \"/coursedata/06-subwords/gum_train.txt\"\n",
    "# check that the vocabulary length is right\n",
    "assert_equal(len(collect_word_counts(gum_train_path)), 8677)\n",
    "# check that the word count is right\n",
    "assert_equal(collect_word_counts(gum_train_path)['we'], 112)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4db9a8174363efdf8678ecb380534f8",
     "grade": false,
     "grade_id": "cell-41847e0b7837e171",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1.2  <a class=\"anchor\" id=\"subtask_1_1_1\"></a>\n",
    "### Converting words to characters (1 Point)\n",
    "Now, represent each word in your frequency dictionary as a tuple of characters plus a special end-of-word marker '_'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a40d6b8a176061a4b790ccdd5d1c7bd",
     "grade": false,
     "grade_id": "cell-e358614034bbeaad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_chars(vocab):\n",
    "    \"\"\"\n",
    "    Takes in a frequeny dictionary of words in the training corpus\n",
    "    and converts the key words to a tuple of characters plus a special end-of-word symbol '_'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : dictionary\n",
    "        a frequency dictionary of words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    separated_vocab : dictionary \n",
    "        a frequency dictionary of words represented as a tuple of characters \n",
    "        plus a special end-of-word symbol '_'\n",
    "        {('l','o','w','_') : 3}\n",
    "    \"\"\"\n",
    "   \n",
    "    separated_vocab={}\n",
    "    for key in vocab:\n",
    "        newkey=[]\n",
    "        for ch in key:\n",
    "            newkey.append(ch)\n",
    "        newkey.append('_')\n",
    "        nk=tuple(newkey)\n",
    "        separated_vocab[nk]=vocab[key]\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "  \n",
    "    return separated_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a539a8a68bee21b1ed5d78736d65466",
     "grade": true,
     "grade_id": "cell-8efbbf91eee65873",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_freq_vocab = {'low': 5, 'lowest': 2, 'newer': 6, 'wider': 3, 'New': 2}\n",
    "\n",
    "# check that the output of the function is a dict\n",
    "assert_equal(type(convert_to_chars(dummy_freq_vocab)), dict)\n",
    "# check that the keys are tuples\n",
    "assert_equal(type(list(convert_to_chars(dummy_freq_vocab).keys())[0]), tuple)\n",
    "#check that there are no new elements\n",
    "assert_equal(len(convert_to_chars(dummy_freq_vocab)), 5)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('l', 'o', 'w', '_')], 5)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('l', 'o', 'w', 'e', 's', 't', '_')], 2)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('n', 'e', 'w', 'e', 'r', '_')], 6)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('w', 'i', 'd', 'e', 'r', '_')], 3)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('N', 'e', 'w', '_')], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62200ea28884b197c5b2dcb89e890873",
     "grade": false,
     "grade_id": "cell-48322d22d20d67b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 <a class=\"anchor\" id=\"subtask_1_2\"></a>\n",
    "### STEP 2\n",
    "### Collecting symbol pairs frequencies (1 Point)\n",
    "Write a function that takes in a frequency dictionary where keys are words represented as tuples of symbols, and outputs the most frequent pair of symbols in the corpus. In the case, when there are several pairs with the same frequency, return the pair that is earlier alphabetically. \n",
    "\n",
    "For example, if we only have one *l o o k _* and one *l o o p _*  in our frequency dictionary, the fuction should output *l o* as the most frequent pair (it is earlier than *o o* alphabetically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f469bc5c4dbd606fcd5ae0034b4496ad",
     "grade": false,
     "grade_id": "cell-e2a9fb6eb0db7f82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def convertTuple(tup):\n",
    "        # initialize an empty string\n",
    "    str = ''\n",
    "    for item in tup:\n",
    "        str = str + item\n",
    "    return str\n",
    " \n",
    "\n",
    "\n",
    "def get_the_pair_to_merge(vocab_as_symbols):\n",
    "    \"\"\"\n",
    "    Takes in a frequency dictionary, where keys are words represented as tuples of symbols and values are their counts, \n",
    "    and outputs the most frequent pair of symbols in the corpus.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    vocab_as_symbols : dictionary\n",
    "        a frequency dictionary, where keys are words represented as tuples of symbols and values are their counts\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    merge_pair : tuple of strings \n",
    "        the most frequent pair of symbols, a pair to merge\n",
    "    \"\"\"\n",
    "\n",
    "    merge_pair=()\n",
    "    list_couples=[]\n",
    "   \n",
    "    for val in vocab_as_symbols:\n",
    "        count=vocab_as_symbols[val]\n",
    "        for i in range(len(val)-1):\n",
    "            c1=val[i]\n",
    "            c2=val[i+1]\n",
    "            tup=(c1,c2)\n",
    "            for j in range(count):\n",
    "                list_couples.append(tup)\n",
    "    \n",
    "    dic=collections.Counter(list_couples)\n",
    " \n",
    "    \n",
    "    \n",
    "    maxv=-1\n",
    "    listaz=[]\n",
    "    for k in dic:\n",
    "        mp=dic[k]\n",
    "        if (mp>maxv):\n",
    "            maxv=mp\n",
    "            merge_pair=k\n",
    "        elif(mp==maxv):\n",
    "            str1=convertTuple(merge_pair)\n",
    "            str2=convertTuple(k)\n",
    "            if(k[0]<merge_pair[0]):\n",
    "                #print(str2)\n",
    "                #print(str1)\n",
    "                merge_pair=k\n",
    "      \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    return merge_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "461a0855fdf521d9ab853b24e5392ab9",
     "grade": true,
     "grade_id": "cell-74bc7d2d9ec1da10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_freq_vocab_as_symbols = {('l','o','o','k','_') : 1, \n",
    "                               ('l','o','o','p','_') : 1}\n",
    "\n",
    "# check that the output of the function is a tuple\n",
    "assert_equal(type(get_the_pair_to_merge(dummy_freq_vocab_as_symbols)), tuple)\n",
    "# check that the output of the function is a tuple of strings\n",
    "assert_equal(type(get_the_pair_to_merge(dummy_freq_vocab_as_symbols)[0]), str)\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "\n",
    "assert_equal(get_the_pair_to_merge(dummy_freq_vocab_as_symbols), ('l', 'o'))\n",
    "\n",
    "\n",
    "dummy_freq_vocab_as_symbols2 = {('l', 'o', 'w', '_'): 5,\n",
    "                                ('l', 'o', 'w', 'e', 's', 't', '_'): 2,\n",
    "                                ('n', 'e', 'w', 'e', 'r', '_'): 6, \n",
    "                                ('w', 'i', 'd', 'e', 'r', '_'): 3, \n",
    "                                ('n', 'e', 'w', '_'): 2}\n",
    "\n",
    "\n",
    "assert_equal(get_the_pair_to_merge(dummy_freq_vocab_as_symbols2), ('e', 'r'))\n",
    "\n",
    "# check that your functions can merge not only characters\n",
    "\n",
    "dummy_freq_vocab_as_symbols3 = {('lo', 'o', 'k', '_'): 1 , \n",
    "                                ('lo', 'o', 'p','_'): 1}\n",
    "\n",
    "assert_equal(get_the_pair_to_merge(dummy_freq_vocab_as_symbols3), ('lo', 'o'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9faa70b5827043a943b87f6a7dcb76bd",
     "grade": false,
     "grade_id": "cell-eb3e7bd4a0d2d372",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 <a class=\"anchor\" id=\"subtask_1_3\"></a>\n",
    "### STEP 3\n",
    "### Merging the most frequent pair (3 Points)\n",
    "\n",
    "Write a function that takes in a pair of symbols to merge and a frequency dictionary where words are represented as tuples of symbols. It should return a frequency dictionary, where words are still represented as tuples of symbols but the most frequnt pair is now merged in every word.\n",
    "\n",
    "For example, if we want to merge *l* and *o* in our *l o o k _* and *l o o p _*  frequency dictionary, the fuction should output *lo o k _* and *lo o p _*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "580f10b6fe27d9d676fcc2abe4716412",
     "grade": false,
     "grade_id": "cell-7f9d1e7549722c1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def merge(vocab_as_symbols, merge_pair):\n",
    "    \"\"\"Merges the most frequent pair of symbols in all words\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    vocab_as_symbols : dictionary\n",
    "        a frequency dictionary, where keys are words repreqsented as tuples of symbols and values are their counts\n",
    "    merge_pair : tuple\n",
    "        a pair of symbols to merge\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    new_vocab : dictionary\n",
    "        a frequency dictionary, where keys are words represented as tuples of symbols, \n",
    "        with the given pair represented as a new symbol (concatenated pair)\n",
    "    \"\"\"\n",
    "    new_vocab={}\n",
    "    \n",
    "    for k  in vocab_as_symbols:\n",
    "        val=vocab_as_symbols[k]\n",
    "        newk=[]\n",
    "        i=0\n",
    "        while(i<(len(k)-1)):\n",
    "            ch1=k[i]\n",
    "            ch2=k[i+1]\n",
    "            tup=(ch1,ch2)\n",
    "            if(tup==merge_pair):\n",
    "                str1=convertTuple(merge_pair)\n",
    "                newk.append(str1)\n",
    "                i=i+len(tup)\n",
    "                if(i==(len(k)-1)):\n",
    "                    newk.append(k[len(k)-1])         \n",
    "            else:\n",
    "                newk.append(ch1)\n",
    "                i=i+1 \n",
    "                if(i==(len(k)-1)):\n",
    "                    newk.append(ch2)\n",
    "                    \n",
    "            \n",
    "   \n",
    "        \n",
    "        newkt=tuple(newk)\n",
    "        new_vocab[newkt]=val           \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1914d4b9b996cdac52b8aabc966bdb0",
     "grade": true,
     "grade_id": "cell-e71ea4c2a2aee8c2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_vocab_as_symbols = {('l', 'o', 'w', '_'): 5,\n",
    "                          ('l', 'o', 'w', 'e', 's', 't', '_'): 2,\n",
    "                          ('n', 'e', 'w', 'e', 'r', '_'): 6, \n",
    "                          ('w', 'i', 'd', 'e', 'r', '_'): 3, \n",
    "                          ('n', 'e', 'w', '_'): 2}\n",
    "\n",
    "dummy_pair = ('e', 'r')\n",
    "\n",
    "# check that the output of the function is a dictionary\n",
    "assert_equal(type(merge(dummy_vocab_as_symbols, dummy_pair)), dict)\n",
    "# check that the keys are tuples\n",
    "assert_equal(type(list(merge(dummy_vocab_as_symbols, dummy_pair).keys())[0]), tuple)\n",
    "#check that there are no new elements\n",
    "assert_equal(len(merge(dummy_vocab_as_symbols, dummy_pair)), 5)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the pair was merged everywhere\n",
    "assert_equal(list(merge(dummy_vocab_as_symbols, dummy_pair).keys()), [('l', 'o', 'w', '_'), \n",
    "                                                                    ('l', 'o', 'w', 'e', 's', 't', '_'), \n",
    "                                                                    ('n', 'e', 'w', 'er', '_'), \n",
    "                                                                    ('w', 'i', 'd', 'er', '_'), \n",
    "                                                                    ('n', 'e', 'w', '_')])\n",
    "# check that you can handle several merge pairs in a word\n",
    "dummy_vocab_as_symbols2 = {('l','o','o','o','l'): 1, \n",
    "                           ('l','o','o','o','o','l'):1}\n",
    "\n",
    "dummy_pair2 = ('o', 'o')\n",
    "\n",
    "\n",
    "assert_equal(list(merge(dummy_vocab_as_symbols2, dummy_pair2).keys()), [('l', 'oo', 'o', 'l'), \n",
    "                                                                        ('l','oo','oo','l')])\n",
    "\n",
    "# check that you can handle merge pairs of multicharacter symols\n",
    "dummy_vocab_as_symbols3 = {('l','oo','oo','l'): 1, \n",
    "                           ('l','oo','oo','oo','o','l'):1}\n",
    "\n",
    "dummy_pair3 = ('oo', 'oo')\n",
    "\n",
    "\n",
    "assert_equal(list(merge(dummy_vocab_as_symbols3, dummy_pair3).keys()), [('l', 'oooo', 'l'), \n",
    "                                                                        ('l','oooo','oo','o','l')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1d084a68e84b418e9e3a6503d02d318",
     "grade": false,
     "grade_id": "cell-ddba734f24516d1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 <a class=\"anchor\" id=\"subtask_1_4\"></a>\n",
    "### STEP 4\n",
    "### Combining steps 1-3 together (1 Point)\n",
    "Now let's combine steps 1-3 into a function that learns $k$ BPE merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da79b54e93721feda78f1538f7ba4cba",
     "grade": false,
     "grade_id": "cell-ccfb0b9de81867b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def learn_BPE_merges(file_name, k):\n",
    "    \"\"\" Learns k BPE subwords\n",
    "    \n",
    "    STEP 1: Collect a word count dictionary from a file,\n",
    "            represent words as a tuple of their characters plus a special end-of-word symbol '_'\n",
    "    Now k times\n",
    "        STEP 2: Choose the most frequent pair of symbols\n",
    "                add this pair as a new subword unit into a subword vocabulary\n",
    "        STEP 3: Merge the symbols in all words\n",
    "\n",
    "        \n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    file_name : string\n",
    "        a path to a training corpus as a string\n",
    "    k : integer\n",
    "        a number of merges to be learned\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    merges : list of strings\n",
    "        a list of k subwords (symbol merges) in the order they were learned\n",
    "        one merge is a tuple of two symbols in the most frequent pair at step k\n",
    "    \"\"\"\n",
    "    ret1=collect_word_counts(file_name)\n",
    "    ret2=convert_to_chars(ret1)\n",
    "    merges = []\n",
    "    for i in range(k):     \n",
    "        merge_pair=get_the_pair_to_merge(ret2)    \n",
    "        merges.append(merge_pair)\n",
    "        ret2=merge(ret2, merge_pair)\n",
    "     \n",
    "        \n",
    "    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f0850b1c7e27f50581f115fcea5a3a2",
     "grade": true,
     "grade_id": "cell-5bfcbdbe63a3fdff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_corpus_path = \"/coursedata/06-subwords/dummy_corpus.txt\"\n",
    "\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)), list)\n",
    "# check that the output of the function is a list of tuples\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)[0]), tuple)\n",
    "# check that the output of the function is a list of tuples of strings\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)[0][0]), str)\n",
    "#check that there are exactly k merges\n",
    "assert_equal(len(learn_BPE_merges(dummy_corpus_path, 10)), 10)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that 8 dummy merges are correct\n",
    "assert_equal(learn_BPE_merges(dummy_corpus_path, 8), [('e', 'r'),\n",
    "                                                     ('er', '_'),\n",
    "                                                     ('e', 'w'),\n",
    "                                                     ('n', 'ew'),\n",
    "                                                     ('l', 'o'),\n",
    "                                                     ('lo', 'w'),\n",
    "                                                     ('new', 'er_'),\n",
    "                                                     ('low', '_')])\n",
    "\n",
    "# A SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "gum_train_path = \"/coursedata/06-subwords/gum_train.txt\"\n",
    "assert_equal(learn_BPE_merges(gum_train_path, 5), [('e', '_'),\n",
    "                                                   ('s', '_'),\n",
    "                                                   ('t', 'h'),\n",
    "                                                   ('t', '_'),\n",
    "                                                   ('d', '_')])\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03b138b65af0db5ff4b97f7190ff0af2",
     "grade": false,
     "grade_id": "cell-b15a884d8a45d11c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.5-7 <a class=\"anchor\" id=\"subtask_1_5\"></a>\n",
    "### STEPS 5-7\n",
    "### Segmenting a corpus (5 Points)\n",
    "Well, now we can apply what we've learned to tokenize any text into its subwords. Write a function that reads the test corpus and applies the merges we've learned on a training corpus. Note that you will need to adapt your previous functions a little for this. For example, we don't need to count the word frequencies since they don't play any role here anymore.\n",
    "\n",
    "Just a reminder of the steps needed to apply a subword tokenzation to a new (or an old) text:\n",
    "\n",
    "* STEP 5: tokenize a test corpus into words (with the same tokenization algorithm as in training).\n",
    "* STEP 6: represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 7: for every word, apply each merge operation in the order they were learned, remove the end-of-word symbol '_' and return the tokenized text.\n",
    "\n",
    "\n",
    "For the purposes of the exercise, the tokenized text should be a list of strings where strings are words with their subwords separated by whitespaces: ['I', 'lo ok', 'g oo d']. This way it will be easier for you to check how each word is tokenized. In the real application, a tokenized text will be represented just as a list of subwords.\n",
    "\n",
    "\n",
    "Note: don't forget to get rid of the special end-of-word symbol after tokenization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f94859ab9c9e366bff7f6dc79a9fd877",
     "grade": false,
     "grade_id": "cell-bc393db75acac1eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def segment_text(file_name, merges):\n",
    "    \"\"\"\n",
    "    Takes in a path to a text file and lerned BPE merges,\n",
    "    reads the file and tokenizes it into subwords in accorance with the merges.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    file_name : string\n",
    "        a path to a text as a string\n",
    "    merges : list of tuples\n",
    "        a list of k merges in the order they were learned\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    segmented_text - list of strings\n",
    "        a text segmented with BPE\n",
    "        the text is a list of words\n",
    "        where each word is a string with its segments separated by whitespaces:\n",
    "        ['I', 'lo ok', 'g oo d']\n",
    "    \"\"\"\n",
    "    listmerges=[]\n",
    "    fp=open(file_name,'r')\n",
    "    text=fp.read()\n",
    "    fp.close()\n",
    "    tokens=text.split()\n",
    "    lista=[]\n",
    "    for token in tokens:\n",
    "        for ch in token:\n",
    "            lista.append(ch)\n",
    "        lista.append('_')\n",
    "    \n",
    "    segmented_text=[]\n",
    "    sentence=[]\n",
    "    dic={}\n",
    "    for elem in lista:\n",
    "        if(elem=='_'):\n",
    "            sentence.append(elem)\n",
    "            dic[tuple(sentence)]=1\n",
    "            for m in merges:\n",
    "                dic=merge(dic,m)\n",
    "            stri=''\n",
    "            for k in dic:\n",
    "                for t in k:\n",
    "                    if(stri==''):\n",
    "                        stri=t\n",
    "                    elif(t=='_'):\n",
    "                        stri=stri\n",
    "                    else:\n",
    "                        stri=stri+' '+t\n",
    "            leng=len(stri)\n",
    "            if(leng!=0):\n",
    "                if(stri[leng-1]=='_'):\n",
    "                    stri=stri[0:leng-1]\n",
    "            \n",
    "            \n",
    "            segmented_text.append(stri)\n",
    "            sentence=[]\n",
    "            dic={}\n",
    "        else:\n",
    "            sentence.append(elem)\n",
    "  \n",
    "            \n",
    "    \n",
    "   \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    return segmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60682daf4bbfa5025ae8f6c4c735e83d",
     "grade": true,
     "grade_id": "cell-6d91df8425edb415",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_train_path = \"/coursedata/06-subwords/dummy_corpus.txt\"\n",
    "\n",
    "dummy_merges = [('e', 'r'),\n",
    "                ('er', '_'),\n",
    "                ('e', 'w'),\n",
    "                ('n', 'ew'),\n",
    "                ('l', 'o'),\n",
    "                ('lo', 'w'),\n",
    "                ('ma', 'ma')]\n",
    "\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(segment_text(dummy_train_path, dummy_merges)), list)\n",
    "# check that the output of the function is a list of strings\n",
    "assert_equal(type(segment_text(dummy_train_path, dummy_merges)[0]), str)\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the dummy train corpus is segmented the way it should:\n",
    "\n",
    "assert_equal(segment_text(dummy_train_path, dummy_merges), ['low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low e s t',\n",
    "                                                             'low e s t',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'W I D er',\n",
    "                                                             'W I D er',\n",
    "                                                             'W I D er',\n",
    "                                                             'new',\n",
    "                                                             'new'])\n",
    "\n",
    "\n",
    "# check that the dummy test corpus is segmented the way it should:\n",
    "dummy_test_path = \"/coursedata/06-subwords/dummy_test_corpus.txt\"\n",
    "assert_equal(segment_text(dummy_test_path, dummy_merges), ['low er', 'c o o l er'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2239a1bf942e5b13adc1cfff92d38bd",
     "grade": false,
     "grade_id": "cell-3b41d5204748e381",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2 <a class=\"anchor\" id=\"task_2\"></a>\n",
    "## ANALYZE SEGMENTATIONS\n",
    "## 2.1 <a class=\"anchor\" id=\"subtask_2_1\"></a>\n",
    "### Count word OOV (1 Point)\n",
    "Now that we've done with the algorithm, let's see if it will actually help us with the OOV problem.\n",
    "Let's use the same corpus as we did in the previous POS-tagging assignment. We've randomly shuffled the sentences and split the corpus roughly in half. One half will be our training example, and another will be our test example.\n",
    "\n",
    "Analyze:\n",
    "1. The number of word **types** in the vocabulary of the training corpus.\n",
    "2. The number of word **types** in the vocabulary of the test corpus.\n",
    "3. The percentage of word **types** in the test corpus vocabulary that are absent in the training corpus. (what part of test corpus vocabulary is OOV?)\n",
    "\n",
    "Run the cell below, to collect the tokenized corpora (we're splitting the words in the corpora by whitespaces), type in the answer in the next cell.\n",
    "\n",
    "Here, you can add a new cell and make the computations you need there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cfff879d293f8960f52f6a9f6c8e777",
     "grade": false,
     "grade_id": "cell-a3ce2430b488bbe3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/coursedata/06-subwords/gum_train.txt\", 'r') as f:\n",
    "    train = f.read()\n",
    "    train = train.split()\n",
    "\n",
    "with open(\"/coursedata/06-subwords/gum_test.txt\", 'r') as f:\n",
    "    test = f.read()\n",
    "    test = test.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n",
      "8770\n"
     ]
    }
   ],
   "source": [
    "trainlist=[]\n",
    "for word in train:\n",
    "    if(word not in trainlist):\n",
    "        trainlist.append(word)\n",
    "print(len(trainlist))\n",
    "\n",
    "\n",
    "testlist=[]\n",
    "for word in test:\n",
    "    if(word not in testlist):\n",
    "        testlist.append(word)\n",
    "print(len(testlist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdbed017e21685e1461c6d5b91ee05bf",
     "grade": false,
     "grade_id": "cell-6b5793a004cb3f7c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# type in the answer as integer number\n",
    "# For example:\n",
    "# len_of_train_vocab = 234\n",
    "#print(counttrain)\n",
    "\n",
    "len_of_train_vocab = len(trainlist)\n",
    "\n",
    "# type in the answer as integer number\n",
    "# For example:\n",
    "# len_of_test_vocab = 234\n",
    "len_of_test_vocab = len(testlist)\n",
    "\n",
    "# type in the answer as a float number between 0 and 100\n",
    "# For example:\n",
    "# oov_percentage = 90.9\n",
    "flist=[]\n",
    "for w in testlist:\n",
    "    if(w not in trainlist):\n",
    "        flist.append(w)\n",
    "\n",
    "oov_percentage=len(flist)/len(testlist)*100\n",
    "print(oov_percentage)    \n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c09d88df0a79688c9a83b55d856aa9e",
     "grade": true,
     "grade_id": "cell-dc2b01fa475e484e",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# QUESTION 1 tests\n",
    "\n",
    "# checks if your answer is an integer\n",
    "assert_equal(type(len_of_train_vocab), int)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(7000 < len_of_train_vocab < 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53f043c15470ae2bf5f88655d3aa2208",
     "grade": true,
     "grade_id": "cell-f522e06e0c3478c1",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks if your answer is an integer\n",
    "assert_equal(type(len_of_test_vocab), int)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(7000 < len_of_test_vocab < 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0f33f040d534dd518a9f8ecce86b42f",
     "grade": true,
     "grade_id": "cell-1223f6fd86f2005c",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks if your answer is a float\n",
    "assert_equal(type(oov_percentage), float)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(1 < oov_percentage < 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5f2520b8d389ef8df0265ff1556cc42",
     "grade": false,
     "grade_id": "cell-586822c916ee5799",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2 <a class=\"anchor\" id=\"subtask_2_2\"></a>\n",
    "### Count subword OOV (1 Point)\n",
    "\n",
    "Now, let's compare the OOV numbers we've got in the case where the text is tokenized by words and the case when it's tokenized by subwords. \n",
    "\n",
    "Learn 5000 BPE segmentation from the training data, then segment both corpora and compare the vocabulary numbers again. Note that it will take a couple of minutes to run BPE.\n",
    "\n",
    "Analyze:\n",
    "1. The number of subword **types** in the vocabulary of the training corpus.\n",
    "2. The number of subword **types** in the vocabulary of the test corpus.\n",
    "3. The percentage of subword **types** in the test corpus vocabulary that are absent in the training corpus. (what part of test corpus vocabulary is OOV?)\n",
    "\n",
    "\n",
    "Again, you can add a new cell and make the computations you need there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a69503220dc9f4e9320c6e1821ddc46a",
     "grade": false,
     "grade_id": "cell-10da1fe631f3755b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "merges = learn_BPE_merges(\"/coursedata/06-subwords/gum_train.txt\", 5000)\n",
    "segmented_train = segment_text(\"/coursedata/06-subwords/gum_train.txt\", merges)\n",
    "segmented_test = segment_text(\"/coursedata/06-subwords/gum_test.txt\", merges)\n",
    "# represent texts as a list of subwords\n",
    "segmented_train_as_subwords = \" \".join(segmented_train).split(\" \")\n",
    "segmented_test_as_subwords = \" \".join(segmented_test).split(\" \")\n",
    "\n",
    "\n",
    "# double checking that your BPE algorithm is working correctly\n",
    "assert_equal(merges[0], ('e', '_'))\n",
    "assert_equal(merges[1000], ('t', 'ro'))\n",
    "assert_equal(merges[-1], ('des', 'cendants_'))\n",
    "\n",
    "assert_equal(segmented_train[580], 'V ill age')\n",
    "assert_equal(segmented_train[5400], 'laun ching')\n",
    "\n",
    "assert_equal(segmented_test[501], 'C a the dr al')\n",
    "assert_equal(segmented_test[517], 'ho t els')\n",
    "\n",
    "assert(len(segmented_train_as_subwords) == 64166)\n",
    "assert(len(segmented_test_as_subwords) == 72712)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(segmented_test_as_subwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segmented_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segmented_test_as_subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain=[]\n",
    "stest=[]\n",
    "for w in segmented_train_as_subwords:\n",
    "    if(w not in strain):\n",
    "        strain.append(w)\n",
    "\n",
    "for w in segmented_test_as_subwords:\n",
    "    if(w not in stest):\n",
    "        stest.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "935cdbba973ce3c0b534dc2d32691eef",
     "grade": false,
     "grade_id": "cell-e5d749c71ee045a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# type in the answer as an integer number\n",
    "# For example:\n",
    "# len_of_train_sub_vocab = 234\n",
    "len_of_train_sub_vocab = len(strain)\n",
    "print(len_of_train_sub_vocab)\n",
    "# type in the answer as an integer number\n",
    "# For example:\n",
    "# len_of_test_sub_vocab = 234\n",
    "len_of_test_sub_vocab = len(stest)\n",
    "print(len_of_test_sub_vocab)\n",
    "# type in the answer as a float number between 0 and 100\n",
    "# For example:\n",
    "# oov_sub_percentage = 50.9\n",
    "slist=[]\n",
    "for w in stest:\n",
    "    if (w not in strain):\n",
    "        slist.append(w)\n",
    "oov_sub_percentage  = len(slist)/len(stest)*100\n",
    "print(oov_sub_percentage)\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4149bad7495bb75d31814dc3b55700b",
     "grade": true,
     "grade_id": "cell-c29d08ffc3d29073",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 1 tests\n",
    "\n",
    "# checks if your answer is an integer\n",
    "assert_equal(type(len_of_train_sub_vocab), int)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(500 < len_of_train_sub_vocab < 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75507a3bc824971b3b98d08fb2091ecf",
     "grade": true,
     "grade_id": "cell-786b076f34bd1f3f",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks if your answer is an integer\n",
    "assert_equal(type(len_of_test_sub_vocab), int)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(500 < len_of_test_sub_vocab < 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "989dd2107af87f96c82c8d1a2dfd690f",
     "grade": true,
     "grade_id": "cell-3d0f1cd7e311c960",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 3 tests\n",
    "\n",
    "# checks if your answer is a float\n",
    "assert_equal(type(oov_sub_percentage), float)\n",
    "# checks if your answer is remorely correct integer\n",
    "assert(1 < oov_sub_percentage < 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb07596827b30c3df8ae3e9e1dea8535",
     "grade": false,
     "grade_id": "cell-e9c3e0cf6893442b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.3 <a class=\"anchor\" id=\"subtask_2_3\"></a>\n",
    "### Does the segmentation make sense? (1 Point)\n",
    "Now let's look a bit closer at the subwords that we've learned. Take a second to think if the results are what you would expect them to be.\n",
    "\n",
    "1. What are the top 10 most frequent subwords in the segmented test corpus? (in decending frequency order)\n",
    "2. What are the top 5 longest subwords in the segmented test corpus? (sort alphabetically)\n",
    "3. What are the top 5 most frequent lengths of subwords in the test corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b72f4d3acf0ed7fbfaf448f35349083",
     "grade": false,
     "grade_id": "cell-e30eb3fbd26f4157",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# type in the answer as a list of strings\n",
    "# For example:\n",
    "# top_10_by_freq = ['a','b'...]\n",
    "dic=collections.Counter(segmented_test_as_subwords)\n",
    "\n",
    "i=0\n",
    "lista=[]\n",
    "\n",
    "for k in dic:\n",
    "    if(i==10):\n",
    "        break;\n",
    "    lista.append(k)\n",
    "    i=i+1\n",
    "top_10_by_freq = ['', 's', 't', 'k', 'a', 'ed', 'o', 'e', 'es', 'p']\n",
    "print(len(top_10_by_freq))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "lista=[]\n",
    "for k in dic:\n",
    "    lista.append(k)\n",
    "lst2=sorted(lista, key=len, reverse=True)\n",
    "top_5_by_len = lst2[0:5]\n",
    "print(top_5_by_len)\n",
    "# type in the answer as a list of integers\n",
    "# For example:\n",
    "# top_5_freqs_of_lens= [1,2,3...]\n",
    "\n",
    "lista=[]\n",
    "for k in dic:\n",
    "    for i in range(dic[k]):\n",
    "        lista.append(len(k))\n",
    "\n",
    "coll=collections.Counter(lista)\n",
    "print(coll)\n",
    "top_5_freqs_of_lens = ['0','2','1','3','4']\n",
    "print(top_5_freqs_of_lens)\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdea4f3caa706b65d0027f60298ccf22",
     "grade": true,
     "grade_id": "cell-e7786799f7aeacdc",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 1 tests\n",
    "\n",
    "# checks if your answer is a list of strings\n",
    "assert_equal(type(top_10_by_freq), list)\n",
    "assert_equal(type(top_10_by_freq[0]), str)\n",
    "assert_equal(len(top_10_by_freq), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "565863fc5e9d4a052da9d1c1a025ee58",
     "grade": true,
     "grade_id": "cell-7f4032e3c0f62c07",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks if your answer is a list of strings\n",
    "assert_equal(type(top_5_by_len), list)\n",
    "assert_equal(type(top_5_by_len[0]), str)\n",
    "assert_equal(len(top_5_by_len), 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6adadc8550bfdc91726ba1484d2518d",
     "grade": true,
     "grade_id": "cell-68ac266fbfcf1cef",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# QUESTION 3 tests\n",
    "\n",
    "# checks if your answer is a list of integers\n",
    "assert_equal(type(top_5_freqs_of_lens), list)\n",
    "assert_equal(type(top_5_freqs_of_lens[0]), int)\n",
    "assert_equal(len(top_5_freqs_of_lens), 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "057bc9c4dd748319bda8117939a2ebc3",
     "grade": false,
     "grade_id": "cell-f67771f63a0e8969",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.4 <a class=\"anchor\" id=\"subtask_2_4\"></a>\n",
    "### Your thoughts (3 Points)\n",
    "Briefly answer the following questions:\n",
    "\n",
    "1. Describe what happens when you change the k parameter? How to find a good number for k?\n",
    "\n",
    "2. What are the possible NLP applications that can benefit from the tokenization into subwords? Why?\n",
    "\n",
    "3. How the OOV number for our data can be lowered further without changing anything in the segmentation procedure (k stays the same)? Hint: think about the things we used to do in the previous assignments but are not doing in this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "819985af5d5b7176cd09585c696d6ec9",
     "grade": true,
     "grade_id": "cell-18c675e80eea69ed",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. If we vary the k-parameter the number of merges found changes. In particular for our application above subwords with lower length are retrieved in case the k decrease otherwise subwords with higher length are found. A good number of k can be found imposing as a limit that a merge should appear at least 2 times for example. So a tuple of two elements is considered a merge if for example appears 2 times in the text. (2 is an arbitrary number). So when the only merges found appears in x occurences we stop the search.\n",
    "2. For example text classification. In text classification the meaning of a word can be exploited by the meaning of its subwords and consequently we can classify a text based on this. Text translation: suppose that we don't have the translation for a compound word: we can translate it with the translation of of its subwords.\n",
    "3. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12bc0acab657a8c9084802ffd1669e66",
     "grade": false,
     "grade_id": "cell-af66e3f2833de7d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checklist before submission <a class=\"anchor\" id=\"checklist\"></a>\n",
    "### 1\n",
    "To make sure that you didn't forget to import some package or to name some variable, press **Kernel -> Restart** and then **Cell -> Run All**. This way your code will be run exactly in the same order as during the autograding.\n",
    "### 2\n",
    "Click the **Validate** button in the upper menu to check that you haven't missed anything. You might need to run the validation in the terminal because BPE algorithm takes time.\n",
    "### 3\n",
    "To submit the notebook, click on the **jupyterhub** logo in the upper left part of the window, choose the **Assignments** folder, and press **submit**. You can submit multiple times, only the last one counts.\n",
    "### 4\n",
    "Please fill in the feedback form in the [Assignment](https://mycourses.aalto.fi/mod/questionnaire/view.php?id=984937) section of Mycoures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
