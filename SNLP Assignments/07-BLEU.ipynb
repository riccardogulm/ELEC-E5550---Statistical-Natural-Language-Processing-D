{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29a2e30b7fc2aef9061214cc512c228f",
     "grade": false,
     "grade_id": "cell-e823893319cc2093",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 5: Machine Translation Evaluation\n",
    "\n",
    "# Released: 07.03.2023\n",
    "# Deadline: 20.03.2023 at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "871c0bff5de00c3a7fb59ac04c9059ab",
     "grade": false,
     "grade_id": "cell-2b4616209ec110f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this assignment, you will learn how machine translation systems can be automatically evaluated, using the BLEU score.\n",
    "\n",
    "KEYWORDS:\n",
    "* BLEU\n",
    "\n",
    "Do not use external libraries in this assignment. Python standard library is allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d058bcc1b883e9aee84f50f123571ab",
     "grade": false,
     "grade_id": "cell-e1d385c1c09cab25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [TASK 1: BLEU score](#task1)\n",
    "    * [Step 1.1: Match N-grams](#subtask1_1)\n",
    "    * [Step 1.2: Match lengths](#subtask1_2)\n",
    "    * [Step 1.3: Put it together](#subtask1_3)\n",
    "    * [Step 1.4: Reflect on BLEU](#subtask1_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "604663eb04e8bfb8cbe844d42fde780b",
     "grade": false,
     "grade_id": "cell-4f67e9fc3d6c389a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Evaluation is a particularly difficult topic in machine translation. Human evaluations are preferred, but expensive. There are multiple automatic evaluation metrics in use. The most common one is BLEU. In this assignment you will implement BLEU calculation - it is computationally very simple.\n",
    "\n",
    "## TASK 1 <a class=\"anchor\" id=\"task1\"></a>\n",
    "## BLEU score\n",
    "\n",
    "**BLEU**, or bilingual evaluation understudy, was proposed in 2002, here: https://www.aclweb.org/anthology/P02-1040.pdf. Take read through the paper, it is clearly written and provides not just the algorithm, but also context and reasons for the design choices. We will implement the algorithm as described by the paper, so you can refer to it.\n",
    "\n",
    "### High-level overview\n",
    "\n",
    "The essential requirement for BLEU is an annotated test corpus. The test corpus consists of source language segments and multiple (or at least one) reference translations for each segment. The reference translations are made by human translators. Multiple reference translations are used to account for the fact that there is no single correct translation.\n",
    "\n",
    "A machine translation system produces a hypothesis translation for each source segment. The hypothesis is then compared to all the references. BLEU looks for matching N-grams of different lengths N. The more matches, the better. Additionally, BLEU compares the length of the hypothesis with the lengths of the translations - brevity is penalized. These two components are combined into a score between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd14ed4a52b7d407d59494a4fc0d0002",
     "grade": false,
     "grade_id": "cell-5a5cd9018823435c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Match N-grams (3 Points) <a class=\"anchor\" id=\"subtask1_1\"></a>\n",
    "Specifically, BLEU uses a concept called modified precision. The hypothesis n-grams can find a match in any reference - but if the same n-gram occurs multiple times in the hypothesis, the number of matches is clipped to the maximum number of times the n-gram occurs in any single reference. The intuition is simple - the references are likely to have many words and phrases in common. Without the clipping, repeating the same likely words in the hypothesis would yield an inflated BLEU score. The hypothesis \"the the the the the the the\" would find many matches from almost any set of references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9107096988cfded90303e65a8eecf6d4",
     "grade": false,
     "grade_id": "cell-7830e8ece39c4315",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('furiously', 'sleep', 'ideas'), ('sleep', 'ideas', 'green'), ('ideas', 'green', 'colorless')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "You may use this ngram forming implementation in your solution.\n",
    "This returns a generator. If you absolutely need a list, you can\n",
    "simply call\n",
    ">>> list(form_ngrams(...))\n",
    "\"\"\"\n",
    "from collections import deque\n",
    "\n",
    "def form_ngrams(tokens, n):\n",
    "    \"\"\"Forms all ngrams from tokens\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : iterable\n",
    "        Tokens to form n-grams from\n",
    "    n : int\n",
    "        The length of ngrams to form\n",
    "    \n",
    "    Yields\n",
    "    ------\n",
    "    tuple\n",
    "        Yields each ngram as a tuple of tokens.   \n",
    "    \"\"\"\n",
    "    window = deque(maxlen=n)\n",
    "    for i, token in enumerate(tokens, start=1):\n",
    "        window.append(token)\n",
    "        if i >= n:\n",
    "            yield tuple(window)\n",
    "            \n",
    "print(list(form_ngrams([\"furiously\", \"sleep\", \"ideas\", \"green\", \"colorless\"], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c437283958ad8f06d1bee2e5674be686",
     "grade": false,
     "grade_id": "cell-c17a696f8678fb21",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def modified_precision(hypothesis, references, n):\n",
    "    \"\"\"Matches hypothesis N-grams to references at given length N\n",
    "    \n",
    "    Hypothesis and references are given as plain token sequences. This function\n",
    "    forms N-grams of appropriate lengths.\n",
    "    \n",
    "    Count the number of times each N-gram occurs in the hypothesis.\n",
    "    Then take the union of N-gram counts in all the references:\n",
    "    for each N-gram, keep track of the maximum number of times it occurs in a single\n",
    "    reference. This forms the reference counts.\n",
    "    Finally, count the clipped hits - sum each N-gram count in the hypothesis,\n",
    "    but clip those hypothesis counts to the reference counts.\n",
    "    \n",
    "    This returns the hits and number of candidates separately, since those values\n",
    "    are aggregated over the full corpus in BLEU.\n",
    "    The modified precision for this single segment would be clipped_hits / num_candidates\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hypothesis : list of str\n",
    "        List of tokens - the hypothesis translation.\n",
    "        E.G. [\"alo\", \"mundo\"]\n",
    "    references : list of lists of str\n",
    "        List of references - and each reference is a list of tokens.\n",
    "        E.G. [[\"hola\", \"mundo\"], [\"alo\", \"tierra\"]]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of candidate N-grams of length N in the hypothesis that found\n",
    "        a match.\n",
    "    int\n",
    "        The total number of candidate N-grams of length N in the hypothesis.\n",
    "    \"\"\"\n",
    "    print('Hypothesis')\n",
    "    print(hypothesis)\n",
    "    print('References')\n",
    "    print(references)\n",
    "    print(n)\n",
    "    print(list(form_ngrams(hypothesis,n)))\n",
    "    hyp_counts = Counter(form_ngrams(hypothesis,n))\n",
    "    \n",
    "    num_candidates = sum(hyp_counts.values())\n",
    "    if num_candidates == 0:\n",
    "        # If hypothesis is shorter than N, we get here.\n",
    "        # We return 0, 1 instead of 0, 0, so that we don't\n",
    "        # run into divide-by-zero errors.\n",
    "        return 0, 1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    lista=[]\n",
    "    for ref in references:\n",
    "        ref_ngrams=list(form_ngrams(ref,n))\n",
    "        lista.append(ref_ngrams)\n",
    "\n",
    "    #lista2=[]\n",
    "    #for elem in lista:\n",
    "    #    for l in elem:\n",
    "    #        lista2.append(l)\n",
    "    #lista=lista2\n",
    "    clipped_hits=0\n",
    "    lista2=[]\n",
    "    \n",
    "    for l in lista:\n",
    "        dic=Counter(l)\n",
    "        lista2.append(dic)\n",
    "    \n",
    "    lista=lista2\n",
    "    print(lista)\n",
    "    \n",
    "    for k in hyp_counts:\n",
    "        v=hyp_counts[k]\n",
    "        n=0\n",
    "        maxnum=0\n",
    "        for d in lista:\n",
    "            n=0\n",
    "            for k2 in d:\n",
    "                if(k2==k):\n",
    "                    n=d[k2]        \n",
    "            if(n>maxnum):\n",
    "                maxnum=n\n",
    "        if(maxnum<v):\n",
    "            v=maxnum\n",
    "        clipped_hits=clipped_hits+v\n",
    "        \n",
    "    # Return the numerator and denominator separately, since\n",
    "    # they get aggregated over the whole corpus.\n",
    "    print(clipped_hits)\n",
    "    return clipped_hits, num_candidates\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d80b30f9e5f609aba6edd595b534b098",
     "grade": true,
     "grade_id": "cell-abdd55f7362d275f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'd']]\n",
      "1\n",
      "[('a',), ('b',), ('c',), ('d',)]\n",
      "[Counter({('a',): 1, ('b',): 1, ('c',): 1, ('d',): 1})]\n",
      "4\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'd']]\n",
      "2\n",
      "[('a', 'b'), ('b', 'c'), ('c', 'd')]\n",
      "[Counter({('a', 'b'): 1, ('b', 'c'): 1, ('c', 'd'): 1})]\n",
      "3\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'd']]\n",
      "3\n",
      "[('a', 'b', 'c'), ('b', 'c', 'd')]\n",
      "[Counter({('a', 'b', 'c'): 1, ('b', 'c', 'd'): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'd']]\n",
      "4\n",
      "[('a', 'b', 'c', 'd')]\n",
      "[Counter({('a', 'b', 'c', 'd'): 1})]\n",
      "1\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'e']]\n",
      "1\n",
      "[('a',), ('b',), ('c',), ('d',)]\n",
      "[Counter({('a',): 1, ('b',): 1, ('c',): 1, ('e',): 1})]\n",
      "3\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'e']]\n",
      "2\n",
      "[('a', 'b'), ('b', 'c'), ('c', 'd')]\n",
      "[Counter({('a', 'b'): 1, ('b', 'c'): 1, ('c', 'e'): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'e']]\n",
      "3\n",
      "[('a', 'b', 'c'), ('b', 'c', 'd')]\n",
      "[Counter({('a', 'b', 'c'): 1, ('b', 'c', 'e'): 1})]\n",
      "1\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'e']]\n",
      "4\n",
      "[('a', 'b', 'c', 'd')]\n",
      "[Counter({('a', 'b', 'c', 'e'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'e'], ['b', 'c', 'c', 'd']]\n",
      "1\n",
      "[('a',), ('b',), ('c',), ('d',)]\n",
      "[Counter({('a',): 1, ('b',): 1, ('c',): 1, ('e',): 1}), Counter({('c',): 2, ('b',): 1, ('d',): 1})]\n",
      "4\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'e'], ['b', 'c', 'c', 'd']]\n",
      "2\n",
      "[('a', 'b'), ('b', 'c'), ('c', 'd')]\n",
      "[Counter({('a', 'b'): 1, ('b', 'c'): 1, ('c', 'e'): 1}), Counter({('b', 'c'): 1, ('c', 'c'): 1, ('c', 'd'): 1})]\n",
      "3\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'e'], ['b', 'c', 'c', 'd']]\n",
      "3\n",
      "[('a', 'b', 'c'), ('b', 'c', 'd')]\n",
      "[Counter({('a', 'b', 'c'): 1, ('b', 'c', 'e'): 1}), Counter({('b', 'c', 'c'): 1, ('c', 'c', 'd'): 1})]\n",
      "1\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c', 'e'], ['b', 'c', 'c', 'd']]\n",
      "4\n",
      "[('a', 'b', 'c', 'd')]\n",
      "[Counter({('a', 'b', 'c', 'e'): 1}), Counter({('b', 'c', 'c', 'd'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['c', 'c', 'c', 'c']\n",
      "References\n",
      "[['a', 'b', 'c', 'e'], ['b', 'c', 'c', 'd']]\n",
      "1\n",
      "[('c',), ('c',), ('c',), ('c',)]\n",
      "[Counter({('a',): 1, ('b',): 1, ('c',): 1, ('e',): 1}), Counter({('c',): 2, ('b',): 1, ('d',): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "['c', 'c', 'c', 'c']\n",
      "References\n",
      "[['a', 'b', 'c', 'e'], ['b', 'c', 'c', 'd']]\n",
      "2\n",
      "[('c', 'c'), ('c', 'c'), ('c', 'c')]\n",
      "[Counter({('a', 'b'): 1, ('b', 'c'): 1, ('c', 'e'): 1}), Counter({('b', 'c'): 1, ('c', 'c'): 1, ('c', 'd'): 1})]\n",
      "1\n",
      "Hypothesis\n",
      "['c', 'c', 'c', 'c']\n",
      "References\n",
      "[['a', 'b', 'c', 'e'], ['b', 'c', 'c', 'd']]\n",
      "3\n",
      "[('c', 'c', 'c'), ('c', 'c', 'c')]\n",
      "[Counter({('a', 'b', 'c'): 1, ('b', 'c', 'e'): 1}), Counter({('b', 'c', 'c'): 1, ('c', 'c', 'd'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['c', 'c', 'c', 'c']\n",
      "References\n",
      "[['a', 'b', 'c', 'e'], ['b', 'c', 'c', 'd']]\n",
      "4\n",
      "[('c', 'c', 'c', 'c')]\n",
      "[Counter({('a', 'b', 'c', 'e'): 1}), Counter({('b', 'c', 'c', 'd'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['waters', 'destroyed', 'perfume']\n",
      "References\n",
      "[['the', 'city', 'of', 'cologne', 'was', 'destroyed', 'by', 'flood', 'waters']]\n",
      "1\n",
      "[('waters',), ('destroyed',), ('perfume',)]\n",
      "[Counter({('the',): 1, ('city',): 1, ('of',): 1, ('cologne',): 1, ('was',): 1, ('destroyed',): 1, ('by',): 1, ('flood',): 1, ('waters',): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "['waters', 'destroyed', 'perfume']\n",
      "References\n",
      "[['the', 'city', 'of', 'cologne', 'was', 'destroyed', 'by', 'flood', 'waters']]\n",
      "2\n",
      "[('waters', 'destroyed'), ('destroyed', 'perfume')]\n",
      "[Counter({('the', 'city'): 1, ('city', 'of'): 1, ('of', 'cologne'): 1, ('cologne', 'was'): 1, ('was', 'destroyed'): 1, ('destroyed', 'by'): 1, ('by', 'flood'): 1, ('flood', 'waters'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['it', 'is', 'a', 'widely', 'accepted', 'truth', 'that', 'an', 'unmarried', 'wealthy', 'man', 'necessarily', 'needs', 'a', 'wife', 'alongside', 'him']\n",
      "References\n",
      "[['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man', 'in', 'posession', 'of', 'agood', 'fortune', 'must', 'be', 'in', 'want', 'of', 'a', 'wife'], ['everybody', 'knows', 'the', 'rich', 'man', 'necessarily', 'needs', 'a', 'wife']]\n",
      "3\n",
      "[('it', 'is', 'a'), ('is', 'a', 'widely'), ('a', 'widely', 'accepted'), ('widely', 'accepted', 'truth'), ('accepted', 'truth', 'that'), ('truth', 'that', 'an'), ('that', 'an', 'unmarried'), ('an', 'unmarried', 'wealthy'), ('unmarried', 'wealthy', 'man'), ('wealthy', 'man', 'necessarily'), ('man', 'necessarily', 'needs'), ('necessarily', 'needs', 'a'), ('needs', 'a', 'wife'), ('a', 'wife', 'alongside'), ('wife', 'alongside', 'him')]\n",
      "[Counter({('it', 'is', 'a'): 1, ('is', 'a', 'truth'): 1, ('a', 'truth', 'universally'): 1, ('truth', 'universally', 'acknowledged'): 1, ('universally', 'acknowledged', 'that'): 1, ('acknowledged', 'that', 'a'): 1, ('that', 'a', 'single'): 1, ('a', 'single', 'man'): 1, ('single', 'man', 'in'): 1, ('man', 'in', 'posession'): 1, ('in', 'posession', 'of'): 1, ('posession', 'of', 'agood'): 1, ('of', 'agood', 'fortune'): 1, ('agood', 'fortune', 'must'): 1, ('fortune', 'must', 'be'): 1, ('must', 'be', 'in'): 1, ('be', 'in', 'want'): 1, ('in', 'want', 'of'): 1, ('want', 'of', 'a'): 1, ('of', 'a', 'wife'): 1}), Counter({('everybody', 'knows', 'the'): 1, ('knows', 'the', 'rich'): 1, ('the', 'rich', 'man'): 1, ('rich', 'man', 'necessarily'): 1, ('man', 'necessarily', 'needs'): 1, ('necessarily', 'needs', 'a'): 1, ('needs', 'a', 'wife'): 1})]\n",
      "4\n",
      "Hypothesis\n",
      "['it', 'is', 'a', 'widely', 'accepted', 'truth', 'that', 'an', 'unmarried', 'wealthy', 'man', 'necessarily', 'needs', 'a', 'wife', 'alongside', 'him']\n",
      "References\n",
      "[['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man', 'in', 'posession', 'of', 'agood', 'fortune', 'must', 'be', 'in', 'want', 'of', 'a', 'wife'], ['everybody', 'knows', 'the', 'rich', 'man', 'necessarily', 'needs', 'a', 'wife']]\n",
      "4\n",
      "[('it', 'is', 'a', 'widely'), ('is', 'a', 'widely', 'accepted'), ('a', 'widely', 'accepted', 'truth'), ('widely', 'accepted', 'truth', 'that'), ('accepted', 'truth', 'that', 'an'), ('truth', 'that', 'an', 'unmarried'), ('that', 'an', 'unmarried', 'wealthy'), ('an', 'unmarried', 'wealthy', 'man'), ('unmarried', 'wealthy', 'man', 'necessarily'), ('wealthy', 'man', 'necessarily', 'needs'), ('man', 'necessarily', 'needs', 'a'), ('necessarily', 'needs', 'a', 'wife'), ('needs', 'a', 'wife', 'alongside'), ('a', 'wife', 'alongside', 'him')]\n",
      "[Counter({('it', 'is', 'a', 'truth'): 1, ('is', 'a', 'truth', 'universally'): 1, ('a', 'truth', 'universally', 'acknowledged'): 1, ('truth', 'universally', 'acknowledged', 'that'): 1, ('universally', 'acknowledged', 'that', 'a'): 1, ('acknowledged', 'that', 'a', 'single'): 1, ('that', 'a', 'single', 'man'): 1, ('a', 'single', 'man', 'in'): 1, ('single', 'man', 'in', 'posession'): 1, ('man', 'in', 'posession', 'of'): 1, ('in', 'posession', 'of', 'agood'): 1, ('posession', 'of', 'agood', 'fortune'): 1, ('of', 'agood', 'fortune', 'must'): 1, ('agood', 'fortune', 'must', 'be'): 1, ('fortune', 'must', 'be', 'in'): 1, ('must', 'be', 'in', 'want'): 1, ('be', 'in', 'want', 'of'): 1, ('in', 'want', 'of', 'a'): 1, ('want', 'of', 'a', 'wife'): 1}), Counter({('everybody', 'knows', 'the', 'rich'): 1, ('knows', 'the', 'rich', 'man'): 1, ('the', 'rich', 'man', 'necessarily'): 1, ('rich', 'man', 'necessarily', 'needs'): 1, ('man', 'necessarily', 'needs', 'a'): 1, ('necessarily', 'needs', 'a', 'wife'): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "['it', 'is', 'a', 'widely', 'accepted', 'truth', 'that', 'an', 'unmarried', 'wealthy', 'man', 'necessarily', 'needs', 'a', 'wife', 'alongside', 'him']\n",
      "References\n",
      "[['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man', 'in', 'posession', 'of', 'agood', 'fortune', 'must', 'be', 'in', 'want', 'of', 'a', 'wife'], ['everybody', 'knows', 'the', 'rich', 'man', 'necessarily', 'needs', 'a', 'wife']]\n",
      "5\n",
      "[('it', 'is', 'a', 'widely', 'accepted'), ('is', 'a', 'widely', 'accepted', 'truth'), ('a', 'widely', 'accepted', 'truth', 'that'), ('widely', 'accepted', 'truth', 'that', 'an'), ('accepted', 'truth', 'that', 'an', 'unmarried'), ('truth', 'that', 'an', 'unmarried', 'wealthy'), ('that', 'an', 'unmarried', 'wealthy', 'man'), ('an', 'unmarried', 'wealthy', 'man', 'necessarily'), ('unmarried', 'wealthy', 'man', 'necessarily', 'needs'), ('wealthy', 'man', 'necessarily', 'needs', 'a'), ('man', 'necessarily', 'needs', 'a', 'wife'), ('necessarily', 'needs', 'a', 'wife', 'alongside'), ('needs', 'a', 'wife', 'alongside', 'him')]\n",
      "[Counter({('it', 'is', 'a', 'truth', 'universally'): 1, ('is', 'a', 'truth', 'universally', 'acknowledged'): 1, ('a', 'truth', 'universally', 'acknowledged', 'that'): 1, ('truth', 'universally', 'acknowledged', 'that', 'a'): 1, ('universally', 'acknowledged', 'that', 'a', 'single'): 1, ('acknowledged', 'that', 'a', 'single', 'man'): 1, ('that', 'a', 'single', 'man', 'in'): 1, ('a', 'single', 'man', 'in', 'posession'): 1, ('single', 'man', 'in', 'posession', 'of'): 1, ('man', 'in', 'posession', 'of', 'agood'): 1, ('in', 'posession', 'of', 'agood', 'fortune'): 1, ('posession', 'of', 'agood', 'fortune', 'must'): 1, ('of', 'agood', 'fortune', 'must', 'be'): 1, ('agood', 'fortune', 'must', 'be', 'in'): 1, ('fortune', 'must', 'be', 'in', 'want'): 1, ('must', 'be', 'in', 'want', 'of'): 1, ('be', 'in', 'want', 'of', 'a'): 1, ('in', 'want', 'of', 'a', 'wife'): 1}), Counter({('everybody', 'knows', 'the', 'rich', 'man'): 1, ('knows', 'the', 'rich', 'man', 'necessarily'): 1, ('the', 'rich', 'man', 'necessarily', 'needs'): 1, ('rich', 'man', 'necessarily', 'needs', 'a'): 1, ('man', 'necessarily', 'needs', 'a', 'wife'): 1})]\n",
      "1\n",
      "Hypothesis\n",
      "['it', 'is', 'a', 'widely', 'accepted', 'truth', 'that', 'an', 'unmarried', 'wealthy', 'man', 'necessarily', 'needs', 'a', 'wife', 'alongside', 'him']\n",
      "References\n",
      "[['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man', 'in', 'posession', 'of', 'agood', 'fortune', 'must', 'be', 'in', 'want', 'of', 'a', 'wife'], ['everybody', 'knows', 'the', 'rich', 'man', 'necessarily', 'needs', 'a', 'wife']]\n",
      "6\n",
      "[('it', 'is', 'a', 'widely', 'accepted', 'truth'), ('is', 'a', 'widely', 'accepted', 'truth', 'that'), ('a', 'widely', 'accepted', 'truth', 'that', 'an'), ('widely', 'accepted', 'truth', 'that', 'an', 'unmarried'), ('accepted', 'truth', 'that', 'an', 'unmarried', 'wealthy'), ('truth', 'that', 'an', 'unmarried', 'wealthy', 'man'), ('that', 'an', 'unmarried', 'wealthy', 'man', 'necessarily'), ('an', 'unmarried', 'wealthy', 'man', 'necessarily', 'needs'), ('unmarried', 'wealthy', 'man', 'necessarily', 'needs', 'a'), ('wealthy', 'man', 'necessarily', 'needs', 'a', 'wife'), ('man', 'necessarily', 'needs', 'a', 'wife', 'alongside'), ('necessarily', 'needs', 'a', 'wife', 'alongside', 'him')]\n",
      "[Counter({('it', 'is', 'a', 'truth', 'universally', 'acknowledged'): 1, ('is', 'a', 'truth', 'universally', 'acknowledged', 'that'): 1, ('a', 'truth', 'universally', 'acknowledged', 'that', 'a'): 1, ('truth', 'universally', 'acknowledged', 'that', 'a', 'single'): 1, ('universally', 'acknowledged', 'that', 'a', 'single', 'man'): 1, ('acknowledged', 'that', 'a', 'single', 'man', 'in'): 1, ('that', 'a', 'single', 'man', 'in', 'posession'): 1, ('a', 'single', 'man', 'in', 'posession', 'of'): 1, ('single', 'man', 'in', 'posession', 'of', 'agood'): 1, ('man', 'in', 'posession', 'of', 'agood', 'fortune'): 1, ('in', 'posession', 'of', 'agood', 'fortune', 'must'): 1, ('posession', 'of', 'agood', 'fortune', 'must', 'be'): 1, ('of', 'agood', 'fortune', 'must', 'be', 'in'): 1, ('agood', 'fortune', 'must', 'be', 'in', 'want'): 1, ('fortune', 'must', 'be', 'in', 'want', 'of'): 1, ('must', 'be', 'in', 'want', 'of', 'a'): 1, ('be', 'in', 'want', 'of', 'a', 'wife'): 1}), Counter({('everybody', 'knows', 'the', 'rich', 'man', 'necessarily'): 1, ('knows', 'the', 'rich', 'man', 'necessarily', 'needs'): 1, ('the', 'rich', 'man', 'necessarily', 'needs', 'a'): 1, ('rich', 'man', 'necessarily', 'needs', 'a', 'wife'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['waters', 'destroyed', 'perfume']\n",
      "References\n",
      "[['the', 'city', 'of', 'cologne', 'was', 'destroyed', 'by', 'flood', 'waters']]\n",
      "1\n",
      "[('waters',), ('destroyed',), ('perfume',)]\n",
      "[Counter({('the',): 1, ('city',): 1, ('of',): 1, ('cologne',): 1, ('was',): 1, ('destroyed',): 1, ('by',): 1, ('flood',): 1, ('waters',): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "['waters', 'destroyed', 'perfume']\n",
      "References\n",
      "[['the', 'city', 'of', 'cologne', 'was', 'destroyed', 'by', 'flood', 'waters']]\n",
      "2\n",
      "[('waters', 'destroyed'), ('destroyed', 'perfume')]\n",
      "[Counter({('the', 'city'): 1, ('city', 'of'): 1, ('of', 'cologne'): 1, ('cologne', 'was'): 1, ('was', 'destroyed'): 1, ('destroyed', 'by'): 1, ('by', 'flood'): 1, ('flood', 'waters'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['waters', 'destroyed', 'perfume']\n",
      "References\n",
      "[['the', 'city', 'of', 'cologne', 'was', 'destroyed', 'by', 'flood', 'waters']]\n",
      "3\n",
      "[('waters', 'destroyed', 'perfume')]\n",
      "[Counter({('the', 'city', 'of'): 1, ('city', 'of', 'cologne'): 1, ('of', 'cologne', 'was'): 1, ('cologne', 'was', 'destroyed'): 1, ('was', 'destroyed', 'by'): 1, ('destroyed', 'by', 'flood'): 1, ('by', 'flood', 'waters'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['waters', 'destroyed', 'perfume']\n",
      "References\n",
      "[['the', 'city', 'of', 'cologne', 'was', 'destroyed', 'by', 'flood', 'waters']]\n",
      "4\n",
      "[]\n",
      "Hypothesis\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "References\n",
      "[['the', 'cat', 'is', 'on', 'the', 'mat'], ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']]\n",
      "1\n",
      "[('the',), ('the',), ('the',), ('the',), ('the',), ('the',), ('the',)]\n",
      "[Counter({('the',): 2, ('cat',): 1, ('is',): 1, ('on',): 1, ('mat',): 1}), Counter({('there',): 1, ('is',): 1, ('a',): 1, ('cat',): 1, ('on',): 1, ('the',): 1, ('mat',): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "References\n",
      "[['the', 'cat', 'is', 'on', 'the', 'mat'], ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']]\n",
      "2\n",
      "[('the', 'the'), ('the', 'the'), ('the', 'the'), ('the', 'the'), ('the', 'the'), ('the', 'the')]\n",
      "[Counter({('the', 'cat'): 1, ('cat', 'is'): 1, ('is', 'on'): 1, ('on', 'the'): 1, ('the', 'mat'): 1}), Counter({('there', 'is'): 1, ('is', 'a'): 1, ('a', 'cat'): 1, ('cat', 'on'): 1, ('on', 'the'): 1, ('the', 'mat'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "References\n",
      "[['the', 'cat', 'is', 'on', 'the', 'mat'], ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']]\n",
      "3\n",
      "[('the', 'the', 'the'), ('the', 'the', 'the'), ('the', 'the', 'the'), ('the', 'the', 'the'), ('the', 'the', 'the')]\n",
      "[Counter({('the', 'cat', 'is'): 1, ('cat', 'is', 'on'): 1, ('is', 'on', 'the'): 1, ('on', 'the', 'mat'): 1}), Counter({('there', 'is', 'a'): 1, ('is', 'a', 'cat'): 1, ('a', 'cat', 'on'): 1, ('cat', 'on', 'the'): 1, ('on', 'the', 'mat'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "References\n",
      "[['the', 'cat', 'is', 'on', 'the', 'mat'], ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']]\n",
      "4\n",
      "[('the', 'the', 'the', 'the'), ('the', 'the', 'the', 'the'), ('the', 'the', 'the', 'the'), ('the', 'the', 'the', 'the')]\n",
      "[Counter({('the', 'cat', 'is', 'on'): 1, ('cat', 'is', 'on', 'the'): 1, ('is', 'on', 'the', 'mat'): 1}), Counter({('there', 'is', 'a', 'cat'): 1, ('is', 'a', 'cat', 'on'): 1, ('a', 'cat', 'on', 'the'): 1, ('cat', 'on', 'the', 'mat'): 1})]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# Basic sanity check:\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 1), (4, 4))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 2), (3, 3))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 3), (2, 2))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 4), (1, 1))\n",
    "\n",
    "# Small difference:\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 1), (3, 4))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 2), (2, 3))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 3), (1, 2))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 4), (0, 1))\n",
    "\n",
    "# More references:\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 1), (4, 4))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 2), (3, 3))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 3), (1, 2))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 4), (0, 1))\n",
    "\n",
    "# Clipping:\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 1), (2, 4))\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 2), (1, 3))\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 3), (0, 2))\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 4), (0, 1))\n",
    "\n",
    "# With real text:\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"], \n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                1), (2, 3))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"], \n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                2), (0, 2))\n",
    "\n",
    "austen_hypo = [\"it\", \"is\", \"a\", \"widely\", \"accepted\", \"truth\", \n",
    "              \"that\", \"an\", \"unmarried\", \"wealthy\", \"man\", \n",
    "              \"necessarily\", \"needs\", \"a\", \"wife\", \"alongside\", \"him\"]\n",
    "austen_ref = [[\"it\", \"is\", \"a\", \"truth\", \"universally\", \"acknowledged\",\n",
    "               \"that\", \"a\", \"single\", \"man\", \"in\", \"posession\", \"of\",\"a\"\"good\",\"fortune\",\n",
    "               \"must\", \"be\", \"in\", \"want\", \"of\", \"a\", \"wife\"],\n",
    "              [\"everybody\", \"knows\", \"the\",\"rich\",\"man\",\"necessarily\", \"needs\", \"a\", \"wife\"]]\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 3), (4,15))\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 4), (2,14))\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 5), (1,13))\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 6), (0,12))\n",
    "\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                1), (2, 3))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                2), (0, 2))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                3), (0, 1))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                4), (0, 1))\n",
    "\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 1), (2, 7))\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 2), (0, 6))\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 3), (0, 5))\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 4), (0, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9957de7709beea78c09779695d8c0000",
     "grade": false,
     "grade_id": "cell-68c5e02f9fade5ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 Match lengths (1 Point) <a class=\"anchor\" id=\"subtask1_2\"></a>\n",
    "\n",
    "BLEU penalizes short hypotheses. This counter part is needed for precision, because precision by itself encourages adding only the most certain candidates. Typically recall is used as precision's pair, why not here? You can read the original paper to see. Instead of recall, BLEU uses a Brevity Penalty, which is computed over the whole test corpus. Here we just find the values which are aggregated - the hypothesis length and the closest reference length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78ffbf4094d18eee16f0c831294fa900",
     "grade": false,
     "grade_id": "cell-137382ee46d4f47f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def brevity_penalty_values(hypothesis, references):\n",
    "    \"\"\"Computes the length of the hypothesis and find the closest reference length.\n",
    "    \n",
    "    Pick the shortest reference length if there are many equally close\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hypothesis : list of str\n",
    "        List of tokens - the hypothesis translation.\n",
    "        E.G. [\"alo\", \"mundo\"]\n",
    "    references : list of lists of str\n",
    "        List of references - and each reference is a list of tokens.\n",
    "        E.G. [[\"hola\", \"mundo\"], [\"alo\", \"tierra\"]]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The length of the hypothesis\n",
    "    int\n",
    "        The length of reference, which is closest to the hypothesis in length\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    This just considers length in tokens - this doesn't consider N-grams.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    hyplen=len(hypothesis)\n",
    "    closest_reflen=2000000\n",
    "    prevdist=100000\n",
    "    for ref in references:\n",
    "        leng=len(ref)\n",
    "        dist=abs(leng-hyplen)\n",
    "        if(dist<prevdist):\n",
    "            prevdist=dist\n",
    "            closest_reflen=leng\n",
    "        elif(dist==prevdist and leng<closest_reflen):\n",
    "            closest_reflen=leng\n",
    "            \n",
    "        \n",
    " \n",
    "    \n",
    "    return hyplen, closest_reflen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d81ce1f3d284ab75009690e15caadf1",
     "grade": true,
     "grade_id": "cell-31315b4d20e99e6a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# Basic cases:\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\"], [[\"e\",\"f\",\"g\"],[\"a\",\"b\"]]), (3,3))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\"], [[\"e\",\"f\",\"g\",\"h\"],[\"a\",\"b\"]]), (3,2))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\",\"d\"], [[\"e\",\"f\",\"g\",\"h\",\"i\",\"j\"],[\"a\",\"b\"]]), (4,2))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\",\"d\"], [[\"e\",\"f\",\"g\",\"h\"],[\"a\",\"b\"]]), (4,4))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\"], [[\"e\",\"f\",\"g\",\"h\"],[\"a\",\"b\",\"c\",\"d\",\"e\"]]), (2,4))\n",
    "assert_equal(brevity_penalty_values([], [[\"a\",\"b\",\"c\",\"d\",\"e\"],[]]), (0,0))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\"],\n",
    "                                    [[\"r\",\"s\",\"t\",\"u\",\"v\",\"x\",\"y\"],\n",
    "                                     [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\"]]), \n",
    "             (17, 22))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "488a7d02b8a0a849ffc466d4b603c59e",
     "grade": false,
     "grade_id": "cell-771b3a9511d24d16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 Put it together <a class=\"anchor\" id=\"subtask1_3\"></a>\n",
    "\n",
    "The key in BLEU is to aggregate the statistics from above over a large test corpus. BLEU computed on an individual sentence does not correlate well with human scores. Even if this sentence-wise BLEU is computed for all individual sentences in a test corpus and then averaged, the resulting metric is much less useful than when a single BLEU score is computed over the full corpus.\n",
    "\n",
    "The modified precision aggregation happens at each N-gram length separately. Typically, BLEU-4 is computed, meaning maximum N-gram length 4. In that case, 8 precision values are collected: total number of hits and total number of candidates at each each N.\n",
    "\n",
    "The length values just consider length in tokens, so only two values are needed: the total length of the hypotheses and the total length of the best matching reference lengths. The brevity penalty (BP) is then computed as follows:\n",
    "$$\n",
    "BP = 1,~c>r\n",
    "$$\n",
    "$$\n",
    "BP = exp(1-\\dfrac{r}{c}), c\\leq r\n",
    "$$\n",
    "with aggregated hypothesis length $c$ and aggregated closest reference length $r$.\n",
    "\n",
    "The metric is finally computed by:\n",
    "$$\n",
    "BLEU = BP\\times exp\\left( \\sum_n^N \\left[\\dfrac{1}{N}log(p_n)\\right]\\right)\n",
    "$$\n",
    "\n",
    "Here aggregated modified precision at N-gram length $n$ is simply computed as: $p_n = \\dfrac{\\text{Clipped Hits}}{\\text{Total Candidates}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9440b0b38a3363e2c0d8863bed6475b3",
     "grade": false,
     "grade_id": "cell-699f52568aa9243a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use these exp and log operations:\n",
    "from math import exp, log\n",
    "EPSILON = 1e-10  # This is used for smoothing zero counts (already implemented)\n",
    "\n",
    "def BLEU(hypothesis_set, references_set, max_n=4):\n",
    "    \"\"\"Computes BLEU over a corpus\n",
    "    \n",
    "    Use your two earlier functions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect these statistics over the corpus ...\n",
    "    hits_totals = {N: 0 for N in range(1,max_n+1)} # Hits by N-gram N\n",
    "    num_candidates_totals = {N: 0 for N in range(1,max_n+1)} # Num candidates by N\n",
    "    hyplen_total = 0\n",
    "    closest_reflen_total = 0\n",
    "    \n",
    "    \n",
    "  \n",
    "  \n",
    "    \n",
    "    # ... in the following loop:\n",
    "    for hypothesis, references in zip(hypothesis_set, references_set):\n",
    "        for i in range(1,max_n+1):\n",
    "            ret=modified_precision(hypothesis, references, i)\n",
    "            hits=ret[0]\n",
    "            num_candidates=ret[1]\n",
    "            hits_totals[i]=hits+hits_totals[i]\n",
    "            num_candidates_totals[i]=num_candidates+num_candidates_totals[i]\n",
    "            \n",
    "    #print(list_hits)\n",
    "    #print(list_candidates)\n",
    "    # Additionally in BLEU, substituting a low value for zero counts is used\n",
    "    # to avoid log(0) problems.\n",
    "    # Smooth zero counts:\n",
    "    hits_totals = {N: hits if hits > 0 else EPSILON for N, hits in hits_totals.items()}\n",
    "    \n",
    "    # Now, compute brevity penalty based on \n",
    "    # hyplen_total and closest_reflen_totals\n",
    "    # Corner case: if hyplen_total == 0: brevity_penalty = 0.\n",
    "    # YOUR CODE HERE\n",
    "   \n",
    "    \n",
    "    \n",
    "    for hypothesis, references in zip(hypothesis_set, references_set):\n",
    "        ret2=brevity_penalty_values(hypothesis, references);\n",
    "        hyplen_total=hyplen_total+ret2[0]\n",
    "        closest_reflen_total=+closest_reflen_total+ret2[1]\n",
    "        \n",
    "    c=hyplen_total\n",
    "    r=closest_reflen_total\n",
    "    \n",
    "    if(c==0):\n",
    "        bp=0\n",
    "    elif(c>r):\n",
    "        bp=1\n",
    "    else:\n",
    "        bp=exp(1-(r/c))\n",
    "        \n",
    "   \n",
    "    print(bp)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    # Finally compute the precisions, and weight them uniformly by 1 / max_n\n",
    "    # Then multiply by brevity penalty and return the final score.\n",
    "    # YOUR CODE HERE\n",
    "    value=0\n",
    "    for i in range(1,max_n+1):\n",
    "        hits=hits_totals[i]\n",
    "        num_candidates=num_candidates_totals[i]\n",
    "        pn=hits/num_candidates\n",
    "        value=log(pn)+value\n",
    "    value=value/max_n\n",
    "    value=exp(value)\n",
    "    final_score=bp*value\n",
    "    return final_score\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciao\n",
      "Hypothesis\n",
      "['it', ' cannot', ' into ', 'a', ' basis', ' for', ' the ', 'european', ' constitution']\n",
      "References\n",
      "['it', 'can', 'serve', 'as', 'a', 'basis', ' for', ' the ', 'establishment', ' of ', 'a', 'european', 'constitution']\n",
      "1\n",
      "[('it',), (' cannot',), (' into ',), ('a',), (' basis',), (' for',), (' the ',), ('european',), (' constitution',)]\n",
      "[Counter({('i',): 1, ('t',): 1}), Counter({('c',): 1, ('a',): 1, ('n',): 1}), Counter({('e',): 2, ('s',): 1, ('r',): 1, ('v',): 1}), Counter({('a',): 1, ('s',): 1}), Counter({('a',): 1}), Counter({('s',): 2, ('b',): 1, ('a',): 1, ('i',): 1}), Counter({(' ',): 1, ('f',): 1, ('o',): 1, ('r',): 1}), Counter({(' ',): 2, ('t',): 1, ('h',): 1, ('e',): 1}), Counter({('e',): 2, ('s',): 2, ('t',): 2, ('a',): 1, ('b',): 1, ('l',): 1, ('i',): 1, ('h',): 1, ('m',): 1, ('n',): 1}), Counter({(' ',): 2, ('o',): 1, ('f',): 1}), Counter({('a',): 1}), Counter({('e',): 2, ('u',): 1, ('r',): 1, ('o',): 1, ('p',): 1, ('a',): 1, ('n',): 1}), Counter({('t',): 3, ('o',): 2, ('n',): 2, ('i',): 2, ('c',): 1, ('s',): 1, ('u',): 1})]\n",
      "1\n",
      "Hypothesis\n",
      "['it', ' cannot', ' into ', 'a', ' basis', ' for', ' the ', 'european', ' constitution']\n",
      "References\n",
      "['it', 'can', 'serve', 'as', 'a', 'basis', ' for', ' the ', 'establishment', ' of ', 'a', 'european', 'constitution']\n",
      "2\n",
      "[('it', ' cannot'), (' cannot', ' into '), (' into ', 'a'), ('a', ' basis'), (' basis', ' for'), (' for', ' the '), (' the ', 'european'), ('european', ' constitution')]\n",
      "[Counter({('i', 't'): 1}), Counter({('c', 'a'): 1, ('a', 'n'): 1}), Counter({('s', 'e'): 1, ('e', 'r'): 1, ('r', 'v'): 1, ('v', 'e'): 1}), Counter({('a', 's'): 1}), Counter(), Counter({('b', 'a'): 1, ('a', 's'): 1, ('s', 'i'): 1, ('i', 's'): 1}), Counter({(' ', 'f'): 1, ('f', 'o'): 1, ('o', 'r'): 1}), Counter({(' ', 't'): 1, ('t', 'h'): 1, ('h', 'e'): 1, ('e', ' '): 1}), Counter({('e', 's'): 1, ('s', 't'): 1, ('t', 'a'): 1, ('a', 'b'): 1, ('b', 'l'): 1, ('l', 'i'): 1, ('i', 's'): 1, ('s', 'h'): 1, ('h', 'm'): 1, ('m', 'e'): 1, ('e', 'n'): 1, ('n', 't'): 1}), Counter({(' ', 'o'): 1, ('o', 'f'): 1, ('f', ' '): 1}), Counter(), Counter({('e', 'u'): 1, ('u', 'r'): 1, ('r', 'o'): 1, ('o', 'p'): 1, ('p', 'e'): 1, ('e', 'a'): 1, ('a', 'n'): 1}), Counter({('o', 'n'): 2, ('t', 'i'): 2, ('c', 'o'): 1, ('n', 's'): 1, ('s', 't'): 1, ('i', 't'): 1, ('t', 'u'): 1, ('u', 't'): 1, ('i', 'o'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['it', ' cannot', ' into ', 'a', ' basis', ' for', ' the ', 'european', ' constitution']\n",
      "References\n",
      "['it', 'can', 'serve', 'as', 'a', 'basis', ' for', ' the ', 'establishment', ' of ', 'a', 'european', 'constitution']\n",
      "3\n",
      "[('it', ' cannot', ' into '), (' cannot', ' into ', 'a'), (' into ', 'a', ' basis'), ('a', ' basis', ' for'), (' basis', ' for', ' the '), (' for', ' the ', 'european'), (' the ', 'european', ' constitution')]\n",
      "[Counter(), Counter({('c', 'a', 'n'): 1}), Counter({('s', 'e', 'r'): 1, ('e', 'r', 'v'): 1, ('r', 'v', 'e'): 1}), Counter(), Counter(), Counter({('b', 'a', 's'): 1, ('a', 's', 'i'): 1, ('s', 'i', 's'): 1}), Counter({(' ', 'f', 'o'): 1, ('f', 'o', 'r'): 1}), Counter({(' ', 't', 'h'): 1, ('t', 'h', 'e'): 1, ('h', 'e', ' '): 1}), Counter({('e', 's', 't'): 1, ('s', 't', 'a'): 1, ('t', 'a', 'b'): 1, ('a', 'b', 'l'): 1, ('b', 'l', 'i'): 1, ('l', 'i', 's'): 1, ('i', 's', 'h'): 1, ('s', 'h', 'm'): 1, ('h', 'm', 'e'): 1, ('m', 'e', 'n'): 1, ('e', 'n', 't'): 1}), Counter({(' ', 'o', 'f'): 1, ('o', 'f', ' '): 1}), Counter(), Counter({('e', 'u', 'r'): 1, ('u', 'r', 'o'): 1, ('r', 'o', 'p'): 1, ('o', 'p', 'e'): 1, ('p', 'e', 'a'): 1, ('e', 'a', 'n'): 1}), Counter({('c', 'o', 'n'): 1, ('o', 'n', 's'): 1, ('n', 's', 't'): 1, ('s', 't', 'i'): 1, ('t', 'i', 't'): 1, ('i', 't', 'u'): 1, ('t', 'u', 't'): 1, ('u', 't', 'i'): 1, ('t', 'i', 'o'): 1, ('i', 'o', 'n'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['it', ' cannot', ' into ', 'a', ' basis', ' for', ' the ', 'european', ' constitution']\n",
      "References\n",
      "['it', 'can', 'serve', 'as', 'a', 'basis', ' for', ' the ', 'establishment', ' of ', 'a', 'european', 'constitution']\n",
      "4\n",
      "[('it', ' cannot', ' into ', 'a'), (' cannot', ' into ', 'a', ' basis'), (' into ', 'a', ' basis', ' for'), ('a', ' basis', ' for', ' the '), (' basis', ' for', ' the ', 'european'), (' for', ' the ', 'european', ' constitution')]\n",
      "[Counter(), Counter(), Counter({('s', 'e', 'r', 'v'): 1, ('e', 'r', 'v', 'e'): 1}), Counter(), Counter(), Counter({('b', 'a', 's', 'i'): 1, ('a', 's', 'i', 's'): 1}), Counter({(' ', 'f', 'o', 'r'): 1}), Counter({(' ', 't', 'h', 'e'): 1, ('t', 'h', 'e', ' '): 1}), Counter({('e', 's', 't', 'a'): 1, ('s', 't', 'a', 'b'): 1, ('t', 'a', 'b', 'l'): 1, ('a', 'b', 'l', 'i'): 1, ('b', 'l', 'i', 's'): 1, ('l', 'i', 's', 'h'): 1, ('i', 's', 'h', 'm'): 1, ('s', 'h', 'm', 'e'): 1, ('h', 'm', 'e', 'n'): 1, ('m', 'e', 'n', 't'): 1}), Counter({(' ', 'o', 'f', ' '): 1}), Counter(), Counter({('e', 'u', 'r', 'o'): 1, ('u', 'r', 'o', 'p'): 1, ('r', 'o', 'p', 'e'): 1, ('o', 'p', 'e', 'a'): 1, ('p', 'e', 'a', 'n'): 1}), Counter({('c', 'o', 'n', 's'): 1, ('o', 'n', 's', 't'): 1, ('n', 's', 't', 'i'): 1, ('s', 't', 'i', 't'): 1, ('t', 'i', 't', 'u'): 1, ('i', 't', 'u', 't'): 1, ('t', 'u', 't', 'i'): 1, ('u', 't', 'i', 'o'): 1, ('t', 'i', 'o', 'n'): 1})]\n",
      "0\n",
      "1\n",
      "Hypothesis\n",
      "['she', 'can ', 'be', ' used', ' as ', 'a ', 'basis', ' for', ' the', ' installation', ' of', ' a', ' european', ' constitution']\n",
      "References\n",
      "['it', 'can', 'serve', 'as', 'a', 'basis', ' for', ' the ', 'establishment', ' of ', 'a', 'european', 'constitution']\n",
      "1\n",
      "[('she',), ('can ',), ('be',), (' used',), (' as ',), ('a ',), ('basis',), (' for',), (' the',), (' installation',), (' of',), (' a',), (' european',), (' constitution',)]\n",
      "[Counter({('i',): 1, ('t',): 1}), Counter({('c',): 1, ('a',): 1, ('n',): 1}), Counter({('e',): 2, ('s',): 1, ('r',): 1, ('v',): 1}), Counter({('a',): 1, ('s',): 1}), Counter({('a',): 1}), Counter({('s',): 2, ('b',): 1, ('a',): 1, ('i',): 1}), Counter({(' ',): 1, ('f',): 1, ('o',): 1, ('r',): 1}), Counter({(' ',): 2, ('t',): 1, ('h',): 1, ('e',): 1}), Counter({('e',): 2, ('s',): 2, ('t',): 2, ('a',): 1, ('b',): 1, ('l',): 1, ('i',): 1, ('h',): 1, ('m',): 1, ('n',): 1}), Counter({(' ',): 2, ('o',): 1, ('f',): 1}), Counter({('a',): 1}), Counter({('e',): 2, ('u',): 1, ('r',): 1, ('o',): 1, ('p',): 1, ('a',): 1, ('n',): 1}), Counter({('t',): 3, ('o',): 2, ('n',): 2, ('i',): 2, ('c',): 1, ('s',): 1, ('u',): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['she', 'can ', 'be', ' used', ' as ', 'a ', 'basis', ' for', ' the', ' installation', ' of', ' a', ' european', ' constitution']\n",
      "References\n",
      "['it', 'can', 'serve', 'as', 'a', 'basis', ' for', ' the ', 'establishment', ' of ', 'a', 'european', 'constitution']\n",
      "2\n",
      "[('she', 'can '), ('can ', 'be'), ('be', ' used'), (' used', ' as '), (' as ', 'a '), ('a ', 'basis'), ('basis', ' for'), (' for', ' the'), (' the', ' installation'), (' installation', ' of'), (' of', ' a'), (' a', ' european'), (' european', ' constitution')]\n",
      "[Counter({('i', 't'): 1}), Counter({('c', 'a'): 1, ('a', 'n'): 1}), Counter({('s', 'e'): 1, ('e', 'r'): 1, ('r', 'v'): 1, ('v', 'e'): 1}), Counter({('a', 's'): 1}), Counter(), Counter({('b', 'a'): 1, ('a', 's'): 1, ('s', 'i'): 1, ('i', 's'): 1}), Counter({(' ', 'f'): 1, ('f', 'o'): 1, ('o', 'r'): 1}), Counter({(' ', 't'): 1, ('t', 'h'): 1, ('h', 'e'): 1, ('e', ' '): 1}), Counter({('e', 's'): 1, ('s', 't'): 1, ('t', 'a'): 1, ('a', 'b'): 1, ('b', 'l'): 1, ('l', 'i'): 1, ('i', 's'): 1, ('s', 'h'): 1, ('h', 'm'): 1, ('m', 'e'): 1, ('e', 'n'): 1, ('n', 't'): 1}), Counter({(' ', 'o'): 1, ('o', 'f'): 1, ('f', ' '): 1}), Counter(), Counter({('e', 'u'): 1, ('u', 'r'): 1, ('r', 'o'): 1, ('o', 'p'): 1, ('p', 'e'): 1, ('e', 'a'): 1, ('a', 'n'): 1}), Counter({('o', 'n'): 2, ('t', 'i'): 2, ('c', 'o'): 1, ('n', 's'): 1, ('s', 't'): 1, ('i', 't'): 1, ('t', 'u'): 1, ('u', 't'): 1, ('i', 'o'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['she', 'can ', 'be', ' used', ' as ', 'a ', 'basis', ' for', ' the', ' installation', ' of', ' a', ' european', ' constitution']\n",
      "References\n",
      "['it', 'can', 'serve', 'as', 'a', 'basis', ' for', ' the ', 'establishment', ' of ', 'a', 'european', 'constitution']\n",
      "3\n",
      "[('she', 'can ', 'be'), ('can ', 'be', ' used'), ('be', ' used', ' as '), (' used', ' as ', 'a '), (' as ', 'a ', 'basis'), ('a ', 'basis', ' for'), ('basis', ' for', ' the'), (' for', ' the', ' installation'), (' the', ' installation', ' of'), (' installation', ' of', ' a'), (' of', ' a', ' european'), (' a', ' european', ' constitution')]\n",
      "[Counter(), Counter({('c', 'a', 'n'): 1}), Counter({('s', 'e', 'r'): 1, ('e', 'r', 'v'): 1, ('r', 'v', 'e'): 1}), Counter(), Counter(), Counter({('b', 'a', 's'): 1, ('a', 's', 'i'): 1, ('s', 'i', 's'): 1}), Counter({(' ', 'f', 'o'): 1, ('f', 'o', 'r'): 1}), Counter({(' ', 't', 'h'): 1, ('t', 'h', 'e'): 1, ('h', 'e', ' '): 1}), Counter({('e', 's', 't'): 1, ('s', 't', 'a'): 1, ('t', 'a', 'b'): 1, ('a', 'b', 'l'): 1, ('b', 'l', 'i'): 1, ('l', 'i', 's'): 1, ('i', 's', 'h'): 1, ('s', 'h', 'm'): 1, ('h', 'm', 'e'): 1, ('m', 'e', 'n'): 1, ('e', 'n', 't'): 1}), Counter({(' ', 'o', 'f'): 1, ('o', 'f', ' '): 1}), Counter(), Counter({('e', 'u', 'r'): 1, ('u', 'r', 'o'): 1, ('r', 'o', 'p'): 1, ('o', 'p', 'e'): 1, ('p', 'e', 'a'): 1, ('e', 'a', 'n'): 1}), Counter({('c', 'o', 'n'): 1, ('o', 'n', 's'): 1, ('n', 's', 't'): 1, ('s', 't', 'i'): 1, ('t', 'i', 't'): 1, ('i', 't', 'u'): 1, ('t', 'u', 't'): 1, ('u', 't', 'i'): 1, ('t', 'i', 'o'): 1, ('i', 'o', 'n'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['she', 'can ', 'be', ' used', ' as ', 'a ', 'basis', ' for', ' the', ' installation', ' of', ' a', ' european', ' constitution']\n",
      "References\n",
      "['it', 'can', 'serve', 'as', 'a', 'basis', ' for', ' the ', 'establishment', ' of ', 'a', 'european', 'constitution']\n",
      "4\n",
      "[('she', 'can ', 'be', ' used'), ('can ', 'be', ' used', ' as '), ('be', ' used', ' as ', 'a '), (' used', ' as ', 'a ', 'basis'), (' as ', 'a ', 'basis', ' for'), ('a ', 'basis', ' for', ' the'), ('basis', ' for', ' the', ' installation'), (' for', ' the', ' installation', ' of'), (' the', ' installation', ' of', ' a'), (' installation', ' of', ' a', ' european'), (' of', ' a', ' european', ' constitution')]\n",
      "[Counter(), Counter(), Counter({('s', 'e', 'r', 'v'): 1, ('e', 'r', 'v', 'e'): 1}), Counter(), Counter(), Counter({('b', 'a', 's', 'i'): 1, ('a', 's', 'i', 's'): 1}), Counter({(' ', 'f', 'o', 'r'): 1}), Counter({(' ', 't', 'h', 'e'): 1, ('t', 'h', 'e', ' '): 1}), Counter({('e', 's', 't', 'a'): 1, ('s', 't', 'a', 'b'): 1, ('t', 'a', 'b', 'l'): 1, ('a', 'b', 'l', 'i'): 1, ('b', 'l', 'i', 's'): 1, ('l', 'i', 's', 'h'): 1, ('i', 's', 'h', 'm'): 1, ('s', 'h', 'm', 'e'): 1, ('h', 'm', 'e', 'n'): 1, ('m', 'e', 'n', 't'): 1}), Counter({(' ', 'o', 'f', ' '): 1}), Counter(), Counter({('e', 'u', 'r', 'o'): 1, ('u', 'r', 'o', 'p'): 1, ('r', 'o', 'p', 'e'): 1, ('o', 'p', 'e', 'a'): 1, ('p', 'e', 'a', 'n'): 1}), Counter({('c', 'o', 'n', 's'): 1, ('o', 'n', 's', 't'): 1, ('n', 's', 't', 'i'): 1, ('s', 't', 'i', 't'): 1, ('t', 'i', 't', 'u'): 1, ('i', 't', 'u', 't'): 1, ('t', 'u', 't', 'i'): 1, ('u', 't', 'i', 'o'): 1, ('t', 'i', 'o', 'n'): 1})]\n",
      "0\n",
      "1\n",
      "4.264366797818026e-09\n",
      "8.032276872815305e-12\n"
     ]
    }
   ],
   "source": [
    "hyps =[\n",
    "    [\"it\",\" cannot\",\" into \",\"a\",\" basis\",\" for\",\" the \",\"european\",\" constitution\"]\n",
    "]\n",
    "\n",
    "hyps2 =[\n",
    "    [\"she\", \"can \",\"be\",\" used\",\" as \",\"a \",\"basis\",\" for\",\" the\",\" installation\",\" of\",\" a\",\" european\",\" constitution\"]\n",
    "]\n",
    "\n",
    "ref=[\n",
    "    [\"it\", \"can\", \"serve\", \"as\", \"a\" ,\"basis\",\" for\",\" the \",\"establishment\",\" of \",\"a\", \"european\" ,\"constitution\"]\n",
    "]\n",
    "\n",
    "print('ciao')\n",
    "b1=BLEU(hyps1, ref)\n",
    "b2=BLEU(hyps2, ref)\n",
    "print(b1)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1847bfb3a819737add2c376052f7f0c",
     "grade": true,
     "grade_id": "cell-3317cee95e9eda22",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c'], ['a', 'b', 'c', 'd', 'e']]\n",
      "1\n",
      "[('a',), ('b',), ('c',), ('d',)]\n",
      "[Counter({('a',): 1, ('b',): 1, ('c',): 1}), Counter({('a',): 1, ('b',): 1, ('c',): 1, ('d',): 1, ('e',): 1})]\n",
      "4\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c'], ['a', 'b', 'c', 'd', 'e']]\n",
      "2\n",
      "[('a', 'b'), ('b', 'c'), ('c', 'd')]\n",
      "[Counter({('a', 'b'): 1, ('b', 'c'): 1}), Counter({('a', 'b'): 1, ('b', 'c'): 1, ('c', 'd'): 1, ('d', 'e'): 1})]\n",
      "3\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c'], ['a', 'b', 'c', 'd', 'e']]\n",
      "3\n",
      "[('a', 'b', 'c'), ('b', 'c', 'd')]\n",
      "[Counter({('a', 'b', 'c'): 1}), Counter({('a', 'b', 'c'): 1, ('b', 'c', 'd'): 1, ('c', 'd', 'e'): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "['a', 'b', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c'], ['a', 'b', 'c', 'd', 'e']]\n",
      "4\n",
      "[('a', 'b', 'c', 'd')]\n",
      "[Counter(), Counter({('a', 'b', 'c', 'd'): 1, ('b', 'c', 'd', 'e'): 1})]\n",
      "1\n",
      "Hypothesis\n",
      "['colourless', 'green', 'ideas']\n",
      "References\n",
      "[['ideas', 'green', 'colourless', 'sleep'], ['colourless', 'green']]\n",
      "1\n",
      "[('colourless',), ('green',), ('ideas',)]\n",
      "[Counter({('ideas',): 1, ('green',): 1, ('colourless',): 1, ('sleep',): 1}), Counter({('colourless',): 1, ('green',): 1})]\n",
      "3\n",
      "Hypothesis\n",
      "['colourless', 'green', 'ideas']\n",
      "References\n",
      "[['ideas', 'green', 'colourless', 'sleep'], ['colourless', 'green']]\n",
      "2\n",
      "[('colourless', 'green'), ('green', 'ideas')]\n",
      "[Counter({('ideas', 'green'): 1, ('green', 'colourless'): 1, ('colourless', 'sleep'): 1}), Counter({('colourless', 'green'): 1})]\n",
      "1\n",
      "Hypothesis\n",
      "['colourless', 'green', 'ideas']\n",
      "References\n",
      "[['ideas', 'green', 'colourless', 'sleep'], ['colourless', 'green']]\n",
      "3\n",
      "[('colourless', 'green', 'ideas')]\n",
      "[Counter({('ideas', 'green', 'colourless'): 1, ('green', 'colourless', 'sleep'): 1}), Counter()]\n",
      "0\n",
      "Hypothesis\n",
      "['colourless', 'green', 'ideas']\n",
      "References\n",
      "[['ideas', 'green', 'colourless', 'sleep'], ['colourless', 'green']]\n",
      "4\n",
      "[]\n",
      "Hypothesis\n",
      "['a', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c'], ['a', 'b', 'c', 'd', 'e']]\n",
      "1\n",
      "[('a',), ('c',), ('d',)]\n",
      "[Counter({('a',): 1, ('b',): 1, ('c',): 1}), Counter({('a',): 1, ('b',): 1, ('c',): 1, ('d',): 1, ('e',): 1})]\n",
      "3\n",
      "Hypothesis\n",
      "['a', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c'], ['a', 'b', 'c', 'd', 'e']]\n",
      "2\n",
      "[('a', 'c'), ('c', 'd')]\n",
      "[Counter({('a', 'b'): 1, ('b', 'c'): 1}), Counter({('a', 'b'): 1, ('b', 'c'): 1, ('c', 'd'): 1, ('d', 'e'): 1})]\n",
      "1\n",
      "Hypothesis\n",
      "['a', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c'], ['a', 'b', 'c', 'd', 'e']]\n",
      "3\n",
      "[('a', 'c', 'd')]\n",
      "[Counter({('a', 'b', 'c'): 1}), Counter({('a', 'b', 'c'): 1, ('b', 'c', 'd'): 1, ('c', 'd', 'e'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['a', 'c', 'd']\n",
      "References\n",
      "[['a', 'b', 'c'], ['a', 'b', 'c', 'd', 'e']]\n",
      "4\n",
      "[]\n",
      "1\n",
      "Hypothesis\n",
      "['waters', 'destroyed', 'perfume']\n",
      "References\n",
      "[['the', 'city', 'of', 'cologne', 'was', 'destroyed', 'by', 'flood', 'waters']]\n",
      "1\n",
      "[('waters',), ('destroyed',), ('perfume',)]\n",
      "[Counter({('the',): 1, ('city',): 1, ('of',): 1, ('cologne',): 1, ('was',): 1, ('destroyed',): 1, ('by',): 1, ('flood',): 1, ('waters',): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "['waters', 'destroyed', 'perfume']\n",
      "References\n",
      "[['the', 'city', 'of', 'cologne', 'was', 'destroyed', 'by', 'flood', 'waters']]\n",
      "2\n",
      "[('waters', 'destroyed'), ('destroyed', 'perfume')]\n",
      "[Counter({('the', 'city'): 1, ('city', 'of'): 1, ('of', 'cologne'): 1, ('cologne', 'was'): 1, ('was', 'destroyed'): 1, ('destroyed', 'by'): 1, ('by', 'flood'): 1, ('flood', 'waters'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['waters', 'destroyed', 'perfume']\n",
      "References\n",
      "[['the', 'city', 'of', 'cologne', 'was', 'destroyed', 'by', 'flood', 'waters']]\n",
      "3\n",
      "[('waters', 'destroyed', 'perfume')]\n",
      "[Counter({('the', 'city', 'of'): 1, ('city', 'of', 'cologne'): 1, ('of', 'cologne', 'was'): 1, ('cologne', 'was', 'destroyed'): 1, ('was', 'destroyed', 'by'): 1, ('destroyed', 'by', 'flood'): 1, ('by', 'flood', 'waters'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['waters', 'destroyed', 'perfume']\n",
      "References\n",
      "[['the', 'city', 'of', 'cologne', 'was', 'destroyed', 'by', 'flood', 'waters']]\n",
      "4\n",
      "[]\n",
      "Hypothesis\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "References\n",
      "[['the', 'cat', 'is', 'on', 'the', 'mat'], ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']]\n",
      "1\n",
      "[('the',), ('the',), ('the',), ('the',), ('the',), ('the',), ('the',)]\n",
      "[Counter({('the',): 2, ('cat',): 1, ('is',): 1, ('on',): 1, ('mat',): 1}), Counter({('there',): 1, ('is',): 1, ('a',): 1, ('cat',): 1, ('on',): 1, ('the',): 1, ('mat',): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "References\n",
      "[['the', 'cat', 'is', 'on', 'the', 'mat'], ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']]\n",
      "2\n",
      "[('the', 'the'), ('the', 'the'), ('the', 'the'), ('the', 'the'), ('the', 'the'), ('the', 'the')]\n",
      "[Counter({('the', 'cat'): 1, ('cat', 'is'): 1, ('is', 'on'): 1, ('on', 'the'): 1, ('the', 'mat'): 1}), Counter({('there', 'is'): 1, ('is', 'a'): 1, ('a', 'cat'): 1, ('cat', 'on'): 1, ('on', 'the'): 1, ('the', 'mat'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "References\n",
      "[['the', 'cat', 'is', 'on', 'the', 'mat'], ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']]\n",
      "3\n",
      "[('the', 'the', 'the'), ('the', 'the', 'the'), ('the', 'the', 'the'), ('the', 'the', 'the'), ('the', 'the', 'the')]\n",
      "[Counter({('the', 'cat', 'is'): 1, ('cat', 'is', 'on'): 1, ('is', 'on', 'the'): 1, ('on', 'the', 'mat'): 1}), Counter({('there', 'is', 'a'): 1, ('is', 'a', 'cat'): 1, ('a', 'cat', 'on'): 1, ('cat', 'on', 'the'): 1, ('on', 'the', 'mat'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "References\n",
      "[['the', 'cat', 'is', 'on', 'the', 'mat'], ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']]\n",
      "4\n",
      "[('the', 'the', 'the', 'the'), ('the', 'the', 'the', 'the'), ('the', 'the', 'the', 'the'), ('the', 'the', 'the', 'the')]\n",
      "[Counter({('the', 'cat', 'is', 'on'): 1, ('cat', 'is', 'on', 'the'): 1, ('is', 'on', 'the', 'mat'): 1}), Counter({('there', 'is', 'a', 'cat'): 1, ('is', 'a', 'cat', 'on'): 1, ('a', 'cat', 'on', 'the'): 1, ('cat', 'on', 'the', 'mat'): 1})]\n",
      "0\n",
      "Hypothesis\n",
      "['it', 'is', 'a', 'widely', 'accepted', 'truth', 'that', 'an', 'unmarried', 'wealthy', 'man', 'necessarily', 'needs', 'a', 'wife', 'alongside', 'him']\n",
      "References\n",
      "[['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man', 'in', 'posession', 'of', 'agood', 'fortune', 'must', 'be', 'in', 'want', 'of', 'a', 'wife'], ['everybody', 'knows', 'the', 'rich', 'man', 'necessarily', 'needs', 'a', 'wife']]\n",
      "1\n",
      "[('it',), ('is',), ('a',), ('widely',), ('accepted',), ('truth',), ('that',), ('an',), ('unmarried',), ('wealthy',), ('man',), ('necessarily',), ('needs',), ('a',), ('wife',), ('alongside',), ('him',)]\n",
      "[Counter({('a',): 3, ('in',): 2, ('of',): 2, ('it',): 1, ('is',): 1, ('truth',): 1, ('universally',): 1, ('acknowledged',): 1, ('that',): 1, ('single',): 1, ('man',): 1, ('posession',): 1, ('agood',): 1, ('fortune',): 1, ('must',): 1, ('be',): 1, ('want',): 1, ('wife',): 1}), Counter({('everybody',): 1, ('knows',): 1, ('the',): 1, ('rich',): 1, ('man',): 1, ('necessarily',): 1, ('needs',): 1, ('a',): 1, ('wife',): 1})]\n",
      "10\n",
      "Hypothesis\n",
      "['it', 'is', 'a', 'widely', 'accepted', 'truth', 'that', 'an', 'unmarried', 'wealthy', 'man', 'necessarily', 'needs', 'a', 'wife', 'alongside', 'him']\n",
      "References\n",
      "[['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man', 'in', 'posession', 'of', 'agood', 'fortune', 'must', 'be', 'in', 'want', 'of', 'a', 'wife'], ['everybody', 'knows', 'the', 'rich', 'man', 'necessarily', 'needs', 'a', 'wife']]\n",
      "2\n",
      "[('it', 'is'), ('is', 'a'), ('a', 'widely'), ('widely', 'accepted'), ('accepted', 'truth'), ('truth', 'that'), ('that', 'an'), ('an', 'unmarried'), ('unmarried', 'wealthy'), ('wealthy', 'man'), ('man', 'necessarily'), ('necessarily', 'needs'), ('needs', 'a'), ('a', 'wife'), ('wife', 'alongside'), ('alongside', 'him')]\n",
      "[Counter({('it', 'is'): 1, ('is', 'a'): 1, ('a', 'truth'): 1, ('truth', 'universally'): 1, ('universally', 'acknowledged'): 1, ('acknowledged', 'that'): 1, ('that', 'a'): 1, ('a', 'single'): 1, ('single', 'man'): 1, ('man', 'in'): 1, ('in', 'posession'): 1, ('posession', 'of'): 1, ('of', 'agood'): 1, ('agood', 'fortune'): 1, ('fortune', 'must'): 1, ('must', 'be'): 1, ('be', 'in'): 1, ('in', 'want'): 1, ('want', 'of'): 1, ('of', 'a'): 1, ('a', 'wife'): 1}), Counter({('everybody', 'knows'): 1, ('knows', 'the'): 1, ('the', 'rich'): 1, ('rich', 'man'): 1, ('man', 'necessarily'): 1, ('necessarily', 'needs'): 1, ('needs', 'a'): 1, ('a', 'wife'): 1})]\n",
      "6\n",
      "Hypothesis\n",
      "['it', 'is', 'a', 'widely', 'accepted', 'truth', 'that', 'an', 'unmarried', 'wealthy', 'man', 'necessarily', 'needs', 'a', 'wife', 'alongside', 'him']\n",
      "References\n",
      "[['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man', 'in', 'posession', 'of', 'agood', 'fortune', 'must', 'be', 'in', 'want', 'of', 'a', 'wife'], ['everybody', 'knows', 'the', 'rich', 'man', 'necessarily', 'needs', 'a', 'wife']]\n",
      "3\n",
      "[('it', 'is', 'a'), ('is', 'a', 'widely'), ('a', 'widely', 'accepted'), ('widely', 'accepted', 'truth'), ('accepted', 'truth', 'that'), ('truth', 'that', 'an'), ('that', 'an', 'unmarried'), ('an', 'unmarried', 'wealthy'), ('unmarried', 'wealthy', 'man'), ('wealthy', 'man', 'necessarily'), ('man', 'necessarily', 'needs'), ('necessarily', 'needs', 'a'), ('needs', 'a', 'wife'), ('a', 'wife', 'alongside'), ('wife', 'alongside', 'him')]\n",
      "[Counter({('it', 'is', 'a'): 1, ('is', 'a', 'truth'): 1, ('a', 'truth', 'universally'): 1, ('truth', 'universally', 'acknowledged'): 1, ('universally', 'acknowledged', 'that'): 1, ('acknowledged', 'that', 'a'): 1, ('that', 'a', 'single'): 1, ('a', 'single', 'man'): 1, ('single', 'man', 'in'): 1, ('man', 'in', 'posession'): 1, ('in', 'posession', 'of'): 1, ('posession', 'of', 'agood'): 1, ('of', 'agood', 'fortune'): 1, ('agood', 'fortune', 'must'): 1, ('fortune', 'must', 'be'): 1, ('must', 'be', 'in'): 1, ('be', 'in', 'want'): 1, ('in', 'want', 'of'): 1, ('want', 'of', 'a'): 1, ('of', 'a', 'wife'): 1}), Counter({('everybody', 'knows', 'the'): 1, ('knows', 'the', 'rich'): 1, ('the', 'rich', 'man'): 1, ('rich', 'man', 'necessarily'): 1, ('man', 'necessarily', 'needs'): 1, ('necessarily', 'needs', 'a'): 1, ('needs', 'a', 'wife'): 1})]\n",
      "4\n",
      "Hypothesis\n",
      "['it', 'is', 'a', 'widely', 'accepted', 'truth', 'that', 'an', 'unmarried', 'wealthy', 'man', 'necessarily', 'needs', 'a', 'wife', 'alongside', 'him']\n",
      "References\n",
      "[['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man', 'in', 'posession', 'of', 'agood', 'fortune', 'must', 'be', 'in', 'want', 'of', 'a', 'wife'], ['everybody', 'knows', 'the', 'rich', 'man', 'necessarily', 'needs', 'a', 'wife']]\n",
      "4\n",
      "[('it', 'is', 'a', 'widely'), ('is', 'a', 'widely', 'accepted'), ('a', 'widely', 'accepted', 'truth'), ('widely', 'accepted', 'truth', 'that'), ('accepted', 'truth', 'that', 'an'), ('truth', 'that', 'an', 'unmarried'), ('that', 'an', 'unmarried', 'wealthy'), ('an', 'unmarried', 'wealthy', 'man'), ('unmarried', 'wealthy', 'man', 'necessarily'), ('wealthy', 'man', 'necessarily', 'needs'), ('man', 'necessarily', 'needs', 'a'), ('necessarily', 'needs', 'a', 'wife'), ('needs', 'a', 'wife', 'alongside'), ('a', 'wife', 'alongside', 'him')]\n",
      "[Counter({('it', 'is', 'a', 'truth'): 1, ('is', 'a', 'truth', 'universally'): 1, ('a', 'truth', 'universally', 'acknowledged'): 1, ('truth', 'universally', 'acknowledged', 'that'): 1, ('universally', 'acknowledged', 'that', 'a'): 1, ('acknowledged', 'that', 'a', 'single'): 1, ('that', 'a', 'single', 'man'): 1, ('a', 'single', 'man', 'in'): 1, ('single', 'man', 'in', 'posession'): 1, ('man', 'in', 'posession', 'of'): 1, ('in', 'posession', 'of', 'agood'): 1, ('posession', 'of', 'agood', 'fortune'): 1, ('of', 'agood', 'fortune', 'must'): 1, ('agood', 'fortune', 'must', 'be'): 1, ('fortune', 'must', 'be', 'in'): 1, ('must', 'be', 'in', 'want'): 1, ('be', 'in', 'want', 'of'): 1, ('in', 'want', 'of', 'a'): 1, ('want', 'of', 'a', 'wife'): 1}), Counter({('everybody', 'knows', 'the', 'rich'): 1, ('knows', 'the', 'rich', 'man'): 1, ('the', 'rich', 'man', 'necessarily'): 1, ('rich', 'man', 'necessarily', 'needs'): 1, ('man', 'necessarily', 'needs', 'a'): 1, ('necessarily', 'needs', 'a', 'wife'): 1})]\n",
      "2\n",
      "0.6653730571622696\n",
      "Hypothesis\n",
      "[1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 4]]\n",
      "1\n",
      "[(1,), (2,), (3,), (4,)]\n",
      "[Counter({(1,): 1, (2,): 1, (3,): 1, (4,): 1})]\n",
      "4\n",
      "Hypothesis\n",
      "[1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 4]]\n",
      "2\n",
      "[(1, 2), (2, 3), (3, 4)]\n",
      "[Counter({(1, 2): 1, (2, 3): 1, (3, 4): 1})]\n",
      "3\n",
      "Hypothesis\n",
      "[1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 4]]\n",
      "3\n",
      "[(1, 2, 3), (2, 3, 4)]\n",
      "[Counter({(1, 2, 3): 1, (2, 3, 4): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "[1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 4]]\n",
      "4\n",
      "[(1, 2, 3, 4)]\n",
      "[Counter({(1, 2, 3, 4): 1})]\n",
      "1\n",
      "1.0\n",
      "Hypothesis\n",
      "[5, 1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 4, 5]]\n",
      "1\n",
      "[(5,), (1,), (2,), (3,), (4,)]\n",
      "[Counter({(1,): 1, (2,): 1, (3,): 1, (4,): 1, (5,): 1})]\n",
      "5\n",
      "Hypothesis\n",
      "[5, 1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 4, 5]]\n",
      "2\n",
      "[(5, 1), (1, 2), (2, 3), (3, 4)]\n",
      "[Counter({(1, 2): 1, (2, 3): 1, (3, 4): 1, (4, 5): 1})]\n",
      "3\n",
      "Hypothesis\n",
      "[5, 1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 4, 5]]\n",
      "3\n",
      "[(5, 1, 2), (1, 2, 3), (2, 3, 4)]\n",
      "[Counter({(1, 2, 3): 1, (2, 3, 4): 1, (3, 4, 5): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "[5, 1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 4, 5]]\n",
      "4\n",
      "[(5, 1, 2, 3), (1, 2, 3, 4)]\n",
      "[Counter({(1, 2, 3, 4): 1, (2, 3, 4, 5): 1})]\n",
      "1\n",
      "1.0\n",
      "Hypothesis\n",
      "[]\n",
      "References\n",
      "[['Silence']]\n",
      "1\n",
      "[]\n",
      "Hypothesis\n",
      "[]\n",
      "References\n",
      "[['Silence']]\n",
      "2\n",
      "[]\n",
      "Hypothesis\n",
      "[]\n",
      "References\n",
      "[['Silence']]\n",
      "3\n",
      "[]\n",
      "Hypothesis\n",
      "[]\n",
      "References\n",
      "[['Silence']]\n",
      "4\n",
      "[]\n",
      "0\n",
      "Hypothesis\n",
      "[1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 5]]\n",
      "1\n",
      "[(1,), (2,), (3,), (4,)]\n",
      "[Counter({(1,): 1, (2,): 1, (3,): 1, (5,): 1})]\n",
      "3\n",
      "Hypothesis\n",
      "[1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 5]]\n",
      "2\n",
      "[(1, 2), (2, 3), (3, 4)]\n",
      "[Counter({(1, 2): 1, (2, 3): 1, (3, 5): 1})]\n",
      "2\n",
      "Hypothesis\n",
      "[1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 5]]\n",
      "3\n",
      "[(1, 2, 3), (2, 3, 4)]\n",
      "[Counter({(1, 2, 3): 1, (2, 3, 5): 1})]\n",
      "1\n",
      "Hypothesis\n",
      "[1, 2, 3, 4]\n",
      "References\n",
      "[[1, 2, 3, 5]]\n",
      "4\n",
      "[(1, 2, 3, 4)]\n",
      "[Counter({(1, 2, 3, 5): 1})]\n",
      "0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# Basic corpus input:\n",
    "hyps = [[\"a\", \"b\", \"c\",\"d\"],\n",
    "        [\"colourless\", \"green\", \"ideas\"],\n",
    "       [\"a\",\"c\",\"d\"]]        \n",
    "references = [[[\"a\",\"b\",\"c\"], [\"a\",\"b\",\"c\",\"d\",\"e\"]], \n",
    "              [[\"ideas\", \"green\", \"colourless\", \"sleep\"],[\"colourless\", \"green\"]],\n",
    "              [[\"a\",\"b\",\"c\"], [\"a\",\"b\",\"c\",\"d\",\"e\"]]]\n",
    "assert_almost_equal(BLEU(hyps, references), 0.5873949094699213, 2)\n",
    "# More corpus input:\n",
    "hyps = [[\"waters\", \"destroyed\", \"perfume\"],\n",
    "        [\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "        [\"it\", \"is\", \"a\", \"widely\", \"accepted\", \"truth\", \n",
    "         \"that\", \"an\", \"unmarried\", \"wealthy\", \"man\", \n",
    "         \"necessarily\", \"needs\", \"a\", \"wife\", \"alongside\", \"him\"]]        \n",
    "references = [[[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "              [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]],\n",
    "              [[\"it\",\"is\",\"a\",\"truth\",\"universally\",\"acknowledged\",\n",
    "                \"that\", \"a\", \"single\", \"man\", \"in\", \"posession\", \"of\",\"a\"\"good\",\"fortune\",\n",
    "               \"must\",\"be\",\"in\",\"want\",\"of\",\"a\",\"wife\"],\n",
    "               [\"everybody\", \"knows\", \"the\",\"rich\",\"man\",\"necessarily\", \"needs\", \"a\", \"wife\"]]]\n",
    "assert_almost_equal(BLEU(hyps, references), 0.1502348037292212, 2)\n",
    "# Exactly the same:\n",
    "assert_equal(BLEU([[1,2,3,4]], [[[1,2,3,4]]]), 1.0)\n",
    "# Slightly different:\n",
    "assert_almost_equal(BLEU([[5,1,2,3,4]], [[[1,2,3,4,5]]]), 0.7071067811865475, 2)\n",
    "# No hypothesis:\n",
    "assert_equal(BLEU([[]], [[[\"Silence\"]]]), 0.)\n",
    "# No hits at N=4:\n",
    "assert_almost_equal(BLEU([[1,2,3,4]], [[[1,2,3,5]]]), 0.0022360679774997894, 2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6616436f33e14adab81780a9023ae6c9",
     "grade": false,
     "grade_id": "cell-df2e90f797befcaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 Reflect on BLEU <a class=\"anchor\" id=\"subtask1_4\"></a>\n",
    "Briefly answer the following questions:\n",
    "\n",
    "1. Will providing more references increase the BLEU score? Why?\n",
    "\n",
    "2. Why is the brevity penalty is used instead of the more traditional recall?\n",
    "\n",
    "3. Will a human translator always get a BLEU score of 1? Why?\n",
    "\n",
    "4. Autograding these written questions is impossible. Or is it? How could we use BLEU to autograde these answers? The upside would be to save a lot of teaching assistant time, but what would be some downsides?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "324538de940f16008e5f853338854de3",
     "grade": true,
     "grade_id": "cell-c038d749e95ea6d4",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. Yes. More references increase for sure the blue score since more references means more matching ngrams (or in the worst case  zero matching). We have also this statement in the research paper.\n",
    "\n",
    "2. From the research paper: We have two sentences 'I always invariably perpetually do.' and 'I always do.' Between these sentences using the recall we evaluate as the best translation the first one since it recalls all the three possible translation of the word; however the best translation is the one in which only one alternative is picked. For this reason recall is not the best metric to pair with precision and it' better to use brevity penalty.\n",
    "\n",
    "3. Not necessarily in fact most of the time there not exists a unique translation for a sentence. So for this reason if a human translates a sentence in a different way from the references it doesn't get score 1.  \n",
    "\n",
    "4. We should have as references a possible set of sentences that represent possible right answers. For example: the first and third question are yes or no questions so if there is a matching in the answer with yes or no between candidate and references we should grade partially the answer for first and third question. Autograding why questions is more difficult but we could provide a lot of references that can in some way reflect what we are expecting from the answers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
